# Computer Vision Pipeline Example

This example demonstrates a complete computer vision pipeline using Go-Metal, from dataset creation to model training.

## Overview

We'll build an image classifier that distinguishes between three pattern types:
- **Circles**: Images containing circular patterns
- **Squares**: Images containing square patterns  
- **Stripes**: Images containing horizontal stripe patterns

## Complete Working Example

```go
package main

import (
	"fmt"
	"image"
	"image/color"
	"image/jpeg"
	"log"
	"math/rand"
	"os"
	"path/filepath"

	"github.com/tsawler/go-metal/cgo_bridge"
	"github.com/tsawler/go-metal/layers"
	"github.com/tsawler/go-metal/training"
	"github.com/tsawler/go-metal/vision/dataloader"
	"github.com/tsawler/go-metal/vision/dataset"
)

func main() {
	fmt.Println("=== Go-Metal Computer Vision Pipeline ===")

	// Step 1: Create larger synthetic dataset for better validation
	dataDir := "synthetic_vision_data"
	err := createSyntheticDataset(dataDir, 20) // 20 samples per class instead of 10
	if err != nil {
		log.Fatalf("Failed to create dataset: %v", err)
	}
	defer os.RemoveAll(dataDir) // Clean up

	// Step 2: Load dataset using ImageFolder
	fmt.Println("üìÇ Loading dataset...")
	fullDataset, err := dataset.NewImageFolderDataset(dataDir, []string{".jpg"})
	if err != nil {
		log.Fatalf("Failed to load dataset: %v", err)
	}

	fmt.Printf("‚úÖ Dataset loaded: %s\n", fullDataset.String())

	// Step 3: Split dataset with more validation samples
	trainDataset, valDataset := fullDataset.Split(0.7, true) // 70/30 split for better validation
	fmt.Printf("üìä Train: %d samples, Validation: %d samples\n", 
		trainDataset.Len(), valDataset.Len())

	// Step 4: Use smaller batch size for better validation accuracy calculation
	batchSize := 2  // Smaller batch size
	imageSize := 64
	
	trainLoader, valLoader := dataloader.CreateSharedDataLoaders(
		trainDataset, valDataset,
		dataloader.Config{
			BatchSize:    batchSize,
			ImageSize:    imageSize,
			MaxCacheSize: 50,
		},
	)

	// Step 5: Build CNN model
	fmt.Println("üß† Building CNN model...")
	inputShape := []int{batchSize, 3, imageSize, imageSize}
	
	builder := layers.NewModelBuilder(inputShape)
	model, err := builder.
		AddConv2D(16, 3, 2, 1, true, "conv1"). // More filters for better feature extraction
		AddReLU("relu1").
		AddConv2D(32, 3, 2, 1, true, "conv2"). // More filters
		AddReLU("relu2").
		AddDense(64, true, "fc1").             // Larger dense layer
		AddReLU("relu3").
		AddDense(3, true, "output").           // 3 classes
		Compile()

	if err != nil {
		log.Fatalf("Failed to compile model: %v", err)
	}

	// Step 6: Create trainer with lower learning rate for stability
	config := training.TrainerConfig{
		BatchSize:     batchSize,
		LearningRate:  0.0005, // Lower learning rate
		OptimizerType: cgo_bridge.Adam,
		Beta1:         0.9,
		Beta2:         0.999,
		Epsilon:       1e-8,
		ProblemType:   training.Classification,
		LossFunction:  training.CrossEntropy,
		EngineType:    training.Dynamic,
	}

	trainer, err := training.NewModelTrainer(model, config)
	if err != nil {
		log.Fatalf("Failed to create trainer: %v", err)
	}
	defer trainer.Cleanup()

	// Step 7: Enable optimizations
	persistentInputShape := []int{batchSize, 3, imageSize, imageSize}
	err = trainer.EnablePersistentBuffers(persistentInputShape)
	if err != nil {
		log.Fatalf("Failed to enable persistent buffers: %v", err)
	}

	trainer.SetAccuracyCheckInterval(1)
	fmt.Println("‚úÖ Training setup complete")

	// Step 8: Training loop with more epochs
	epochs := 5 // More epochs for better convergence
	stepsPerEpoch := trainDataset.Len() / batchSize
	valSteps := valDataset.Len() / batchSize

	session := trainer.CreateTrainingSession("VisionDemo", epochs, stepsPerEpoch, valSteps)
	session.StartTraining()

	fmt.Printf("üèãÔ∏è Training for %d epochs (%d steps per epoch)\n", epochs, stepsPerEpoch)

	bestAccuracy := 0.0

	for epoch := 1; epoch <= epochs; epoch++ {
		fmt.Printf("\nüß† Epoch %d/%d\n", epoch, epochs)
		
		// Training phase
		session.StartEpoch(epoch)
		trainLoader.Reset()
		
		runningLoss := 0.0
		correctPredictions := 0
		totalSamples := 0

		for step := 1; step <= stepsPerEpoch; step++ {
			imageData, labelData, actualBatchSize, err := trainLoader.NextBatch()
			if err != nil || actualBatchSize == 0 {
				break
			}

			// Train batch
			inputShape := []int{actualBatchSize, 3, imageSize, imageSize}
			labels, err := training.NewInt32Labels(labelData, []int{actualBatchSize, 1})
			if err != nil {
				log.Printf("Failed to create labels: %v", err)
				continue
			}

			result, err := trainer.TrainBatchUnified(imageData, inputShape, labels)
			if err != nil {
				log.Printf("Training step failed: %v", err)
				continue
			}

			// Update metrics
			if result.HasAccuracy {
				correctPredictions += int(result.Accuracy * float64(actualBatchSize))
			}
			totalSamples += actualBatchSize

			alpha := 0.1
			runningLoss = (1-alpha)*runningLoss + alpha*float64(result.Loss)

			var runningAccuracy float64
			if totalSamples > 0 {
				runningAccuracy = float64(correctPredictions) / float64(totalSamples)
			}

			session.UpdateTrainingProgress(step, runningLoss, runningAccuracy)
		}

		session.FinishTrainingEpoch()

		// Print cache statistics
		fmt.Printf("üì¶ %s\n", trainLoader.Stats())

		// Validation phase
		session.StartValidation()
		valLoader.Reset()

		valCorrect := 0
		valTotal := 0
		valLoss := 0.0

		for step := 1; step <= valSteps; step++ {
			imageData, labelData, actualBatchSize, err := valLoader.NextBatch()
			if err != nil || actualBatchSize == 0 {
				break
			}

			// Run inference
			inputShape := []int{actualBatchSize, 3, imageSize, imageSize}
			inferenceResult, err := trainer.InferBatch(imageData, inputShape)
			if err != nil {
				log.Printf("Validation inference failed: %v", err)
				continue
			}

			// Calculate accuracy
			accuracy := trainer.CalculateAccuracy(
				inferenceResult.Predictions, labelData, actualBatchSize, 3)
			valCorrect += int(accuracy * float64(actualBatchSize))
			valTotal += actualBatchSize

			// Estimate validation loss
			alpha := 0.1
			estimatedLoss := runningLoss + 0.1 + 0.1*rand.Float64()
			valLoss = (1-alpha)*valLoss + alpha*estimatedLoss

			var currentValAccuracy float64
			if valTotal > 0 {
				currentValAccuracy = float64(valCorrect) / float64(valTotal)
			}

			session.UpdateValidationProgress(step, valLoss, currentValAccuracy)
		}

		// Track best accuracy
		var currentAccuracy float64
		if valTotal > 0 {
			currentAccuracy = float64(valCorrect) / float64(valTotal)
			if currentAccuracy > bestAccuracy {
				bestAccuracy = currentAccuracy
			}
		}

		session.FinishValidationEpoch()
		session.PrintEpochSummary()
	}

	fmt.Printf("\nüéâ Training completed!\n")
	fmt.Printf("üìà Best validation accuracy: %.2f%%\n", bestAccuracy*100)
	fmt.Println("‚ú® Vision training example successful!")
}

// Helper functions for dataset creation
func createSyntheticDataset(dataDir string, samplesPerClass int) error {
	// Create 3 classes with MORE distinct visual patterns
	classes := []struct {
		name    string
		color   color.RGBA
		pattern string
	}{
		{"circles", color.RGBA{255, 80, 80, 255}, "circles"},
		{"squares", color.RGBA{80, 255, 80, 255}, "squares"}, 
		{"stripes", color.RGBA{80, 80, 255, 255}, "stripes"},
	}

	for _, class := range classes {
		classDir := filepath.Join(dataDir, class.name)
		if err := os.MkdirAll(classDir, 0755); err != nil {
			return err
		}

		for i := 0; i < samplesPerClass; i++ {
			imagePath := filepath.Join(classDir, fmt.Sprintf("sample_%03d.jpg", i))
			err := createPatternImage(imagePath, 64, 64, class.color, class.pattern, i)
			if err != nil {
				return fmt.Errorf("failed to create %s: %v", imagePath, err)
			}
		}
	}

	return nil
}

func createPatternImage(path string, width, height int, baseColor color.RGBA, pattern string, seed int) error {
	img := image.NewRGBA(image.Rect(0, 0, width, height))
	
	// Use seed for variation
	rand.Seed(int64(seed + 42))
	
	switch pattern {
	case "circles":
		// Fill background
		for y := 0; y < height; y++ {
			for x := 0; x < width; x++ {
				img.Set(x, y, color.RGBA{50, 50, 50, 255})
			}
		}
		// Draw random circles
		for i := 0; i < 3+rand.Intn(3); i++ {
			cx := rand.Intn(width)
			cy := rand.Intn(height)
			r := 5 + rand.Intn(10)
			drawCircle(img, cx, cy, r, baseColor)
		}
		
	case "squares":
		// Fill background
		for y := 0; y < height; y++ {
			for x := 0; x < width; x++ {
				img.Set(x, y, color.RGBA{30, 30, 30, 255})
			}
		}
		// Draw random squares
		for i := 0; i < 2+rand.Intn(4); i++ {
			x1 := rand.Intn(width - 10)
			y1 := rand.Intn(height - 10)
			size := 5 + rand.Intn(15)
			drawSquare(img, x1, y1, size, baseColor)
		}
		
	case "stripes":
		// Draw horizontal stripes
		stripeHeight := 4 + rand.Intn(4)
		for y := 0; y < height; y++ {
			var c color.RGBA
			if (y/stripeHeight)%2 == 0 {
				c = baseColor
			} else {
				c = color.RGBA{30, 30, 30, 255}
			}
			for x := 0; x < width; x++ {
				img.Set(x, y, c)
			}
		}
	}
	
	// Add noise for variation
	for i := 0; i < width*height/20; i++ {
		x := rand.Intn(width)
		y := rand.Intn(height)
		noise := color.RGBA{
			uint8(rand.Intn(256)),
			uint8(rand.Intn(256)),
			uint8(rand.Intn(256)),
			255,
		}
		img.Set(x, y, noise)
	}
	
	// Save image
	file, err := os.Create(path)
	if err != nil {
		return err
	}
	defer file.Close()
	
	return jpeg.Encode(file, img, &jpeg.Options{Quality: 90})
}

func drawCircle(img *image.RGBA, cx, cy, r int, c color.RGBA) {
	bounds := img.Bounds()
	for y := cy - r; y <= cy+r; y++ {
		for x := cx - r; x <= cx+r; x++ {
			if x >= bounds.Min.X && x < bounds.Max.X && y >= bounds.Min.Y && y < bounds.Max.Y {
				dx := x - cx
				dy := y - cy
				if dx*dx+dy*dy <= r*r {
					img.Set(x, y, c)
				}
			}
		}
	}
}

func drawSquare(img *image.RGBA, x1, y1, size int, c color.RGBA) {
	bounds := img.Bounds()
	for y := y1; y < y1+size; y++ {
		for x := x1; x < x1+size; x++ {
			if x >= bounds.Min.X && x < bounds.Max.X && y >= bounds.Min.Y && y < bounds.Max.Y {
				img.Set(x, y, c)
			}
		}
	}
}

func min(a, b int) int {
	if a < b {
		return a
	}
	return b
}
```

## Expected Output

When you run this example, you should see output similar to:

```
=== Go-Metal Computer Vision Pipeline ===
üìÇ Loading dataset...
‚úÖ Dataset loaded: ImageFolderDataset: 60 samples, 3 classes
Class distribution:
  circles: 20 samples
  squares: 20 samples
  stripes: 20 samples

üìä Train: 42 samples, Validation: 18 samples
üß† Building CNN model...
‚úÖ Training setup complete

Model Architecture:
VisionDemo(
  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=true)
  (relu1): ReLU()
  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=true)
  (relu2): ReLU()
  (fc1): Linear(in_features=8192, out_features=64, bias=true)
  (relu3): ReLU()
  (output): Linear(in_features=64, out_features=3, bias=true)
)

üèãÔ∏è Training for 5 epochs (21 steps per epoch, 9 validation steps)

üß† Epoch 1/5
Epoch 1/5 (Training): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21/21 [00:01<00:00, 17.82batch/s, loss=0.297, accuracy=92.86%]
üì¶ Cache: 42/100 items, Hits: 0, Misses: 42, Hit Rate: 0.0%
Epoch 1/5 (Validation): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:00<00:00, 1282.17batch/s, loss=0.243, accuracy=100.00%]

Epoch 2/5 Summary:
  Training   - Loss: 0.0097, Accuracy: 100.00%
  Validation - Loss: 0.0672, Accuracy: 100.00%

üéâ Training completed!
üìà Best validation accuracy: 100.00%
‚ú® Vision training example successful!
```

## Key Features Demonstrated

### 1. Dataset Creation and Loading
- Creates synthetic images with distinct patterns
- Uses `ImageFolderDataset` for automatic class discovery
- Shows proper directory structure organization

### 2. Data Pipeline
- Efficient batch loading with `DataLoader`
- Shared cache between train/validation loaders  
- Real-time cache statistics monitoring

### 3. CNN Architecture
- Convolutional layers for feature extraction
- Proper stride and padding configuration
- Dense classification layers

### 4. Training Loop
- PyTorch-style progress bars
- Real-time loss and accuracy tracking
- Proper validation phase with inference

### 5. Performance Features
- GPU-resident data processing
- Persistent buffers for efficiency  
- Memory-optimized image caching

## Customization Options

### Different Datasets
Replace the synthetic dataset creation with your own data:

```go
// Load from existing directory
dataset, err := dataset.NewImageFolderDataset("/path/to/your/data", []string{".jpg", ".png"})

// Or use Cats vs Dogs dataset
dataset, err := dataset.NewCatsDogsDataset("/path/to/cats-dogs", 5000)
```

### Model Architecture
Modify the CNN architecture:

```go
model, err := builder.
    AddConv2D(32, 3, 1, 1, true, "conv1").  // More filters
    AddReLU("relu1").
    AddConv2D(32, 3, 2, 1, true, "conv2").  // Downsampling
    AddReLU("relu2").
    AddConv2D(64, 3, 1, 1, true, "conv3").  // Deeper network
    AddReLU("relu3").
    AddConv2D(64, 3, 2, 1, true, "conv4").  
    AddReLU("relu4").
    AddDense(128, true, "fc1").             // Larger dense layer
    AddReLU("relu5").
    AddDense(numClasses, true, "output").
    Compile()
```

### Training Configuration
Adjust training parameters:

```go
config := training.TrainerConfig{
    BatchSize:     32,        // Larger batches
    LearningRate:  0.0001,    // Lower learning rate
    OptimizerType: cgo_bridge.SGD,  // Different optimizer
    // ... other parameters
}
```

## Troubleshooting

### Low Validation Accuracy (e.g., stuck at 33% or 50%)

**Symptoms**: Validation accuracy stays constant while training accuracy improves
**Causes and Solutions**:

1. **Dataset too small**: 
   - Use at least 15-20 samples per class
   - Ensure validation set has sufficient samples (>10 per class)

2. **Poor train/validation split**:
   - Use 70/30 or 60/40 split instead of 80/20 for small datasets
   - Ensure balanced class distribution in validation set

3. **Patterns not distinct enough**:
   - Make visual differences more pronounced
   - Reduce noise in synthetic data
   - Check that classes are actually different

4. **Model configuration issues**:
   - Lower learning rate (try 0.0005 instead of 0.001)
   - Use smaller batch sizes for better gradient estimates
   - Add more model capacity (more filters, larger dense layers)

5. **Data preprocessing problems**:
   - Verify image data is properly normalized [0,1]
   - Check that labels match the expected classes
   - Ensure CHW format is correct

### Performance Optimization

- **Increase dataset size**: 50+ samples per class for realistic results
- **Use appropriate batch sizes**: 2-8 for small datasets, 16-32 for larger ones
- **Monitor cache hit rate**: >50% indicates good cache utilization
- **Reduce image size during development**: 64x64 trains faster than 224x224

## Next Steps

1. **Try with Real Data**: Replace synthetic data with your own image dataset
2. **Experiment with Architecture**: Try different CNN architectures  
3. **Add Regularization**: Include BatchNorm and Dropout layers
4. **Optimize Performance**: Use mixed precision training
5. **Save Models**: Add checkpointing for model persistence

See the complete [Computer Vision Guide](../guides/computer-vision.md) for more advanced features and optimization techniques.