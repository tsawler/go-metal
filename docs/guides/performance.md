# Performance Guide

Advanced optimization techniques for maximizing go-metal performance on Apple Silicon.

## üéØ Overview

This guide covers advanced performance optimization strategies to get the maximum speed and efficiency from go-metal on Apple Silicon. You'll learn to leverage Apple's unified memory architecture, optimize GPU utilization, and achieve production-level performance.

## ‚ö° Apple Silicon Performance Fundamentals

### Unified Memory Architecture Advantages

```go
package main

import (
    "fmt"
    "log"
    "time"
    
    "github.com/tsawler/go-metal/cgo_bridge"
    "github.com/tsawler/go-metal/layers"
    "github.com/tsawler/go-metal/memory"
    "github.com/tsawler/go-metal/training"
)

func demonstrateUnifiedMemoryAdvantages() {
    fmt.Println("üß† Unified Memory Architecture Advantages")
    fmt.Println("=========================================")
    
    fmt.Println("\nüéØ Key Benefits:")
    fmt.Println("   ‚Ä¢ Zero-copy data sharing between CPU and GPU")
    fmt.Println("   ‚Ä¢ No PCIe bottleneck (integrated GPU)")
    fmt.Println("   ‚Ä¢ Larger effective GPU memory")
    fmt.Println("   ‚Ä¢ Lower latency memory access")
    
    fmt.Println("\nüìä Performance Implications:")
    fmt.Println("   ‚Ä¢ Larger batch sizes possible")
    fmt.Println("   ‚Ä¢ Reduced memory fragmentation")
    fmt.Println("   ‚Ä¢ Faster data loading")
    fmt.Println("   ‚Ä¢ Better multi-tasking performance")
    
    fmt.Println("\nüîß Optimization Strategies:")
    fmt.Println("   ‚Ä¢ Keep data GPU-resident")
    fmt.Println("   ‚Ä¢ Use larger batch sizes")
    fmt.Println("   ‚Ä¢ Minimize CPU-GPU synchronization")
    fmt.Println("   ‚Ä¢ Leverage memory bandwidth")
}
```

### GPU Utilization Monitoring

```go
func monitorGPUUtilization() {
    fmt.Println("üìä GPU Utilization Monitoring")
    fmt.Println("==============================")
    
    fmt.Println("\nüîç Key Metrics to Track:")
    
    metrics := []struct {
        metric string
        description string
        target string
        tool string
    }{
        {
            "GPU Utilization",
            "Percentage of GPU compute in use",
            "> 80%",
            "Activity Monitor, powermetrics",
        },
        {
            "Memory Usage",
            "GPU memory consumption",
            "< 90% of available",
            "Activity Monitor",
        },
        {
            "Memory Bandwidth",
            "Data transfer rate",
            "High during training",
            "Instruments, powermetrics",
        },
        {
            "Thermal State",
            "GPU temperature and throttling",
            "< 85¬∞C sustained",
            "TG Pro, iStat Menus",
        },
    }
    
    fmt.Printf("%-18s | %-30s | %-15s | %-25s\n",
               "Metric", "Description", "Target", "Monitoring Tool")
    fmt.Println("-------------------|--------------------------------|-----------------|-------------------------")
    
    for _, m := range metrics {
        fmt.Printf("%-18s | %-30s | %-15s | %-25s\n",
                   m.metric, m.description, m.target, m.tool)
    }
    
    fmt.Println("\nüí° Command Line Monitoring:")
    fmt.Println("   # GPU utilization")
    fmt.Println("   sudo powermetrics --samplers gpu_power -n 1")
    fmt.Println("   ")
    fmt.Println("   # Memory usage")
    fmt.Println("   vm_stat | grep 'Pages active'")
}
```

## üöÄ Batch Size Optimization

### Finding Optimal Batch Size

```go
func optimizeBatchSize() {
    fmt.Println("üì¶ Batch Size Optimization")
    fmt.Println("==========================")
    
    // Initialize Metal device and memory manager
    device, err := cgo_bridge.CreateMetalDevice()
    if err != nil {
        log.Fatalf("Failed to create Metal device: %v", err)
    }
    defer cgo_bridge.DestroyMetalDevice(device)
    
    memory.InitializeGlobalMemoryManager(device)
    
    fmt.Println("\nüéØ Batch Size Impact on Performance:")
    
    batchSizes := []int{8, 16, 32, 64, 128, 256}
    
    for _, batchSize := range batchSizes {
        throughput := measureTrainingThroughput(batchSize)
        memoryUsage := estimateMemoryUsage(batchSize)
        
        fmt.Printf("   Batch %3d: %.1f samples/sec, ~%.1fGB memory\n", 
                   batchSize, throughput, memoryUsage)
    }
    
    fmt.Println("\nüìè Batch Size Selection Guidelines:")
    fmt.Println("   ‚Ä¢ Start with 32-64 for most models")
    fmt.Println("   ‚Ä¢ Increase until memory limit reached")
    fmt.Println("   ‚Ä¢ Consider gradient noise vs speed tradeoff")
    fmt.Println("   ‚Ä¢ Use powers of 2 for optimal GPU utilization")
    
    fmt.Println("\n‚öñÔ∏è Trade-offs:")
    fmt.Println("   Larger Batches:")
    fmt.Println("   ‚úÖ Higher GPU utilization")
    fmt.Println("   ‚úÖ Better training stability")
    fmt.Println("   ‚ùå Higher memory usage")
    fmt.Println("   ‚ùå Less gradient noise (may affect generalization)")
    
    fmt.Println("\n   Smaller Batches:")
    fmt.Println("   ‚úÖ Lower memory usage")
    fmt.Println("   ‚úÖ More gradient noise (better exploration)")
    fmt.Println("   ‚ùå Lower GPU utilization")
    fmt.Println("   ‚ùå More training variance")
}

func measureTrainingThroughput(batchSize int) float64 {
    // Simplified throughput estimation
    // Real implementation would run actual training steps
    baseSpeed := 100.0 // samples per second at batch 32
    efficiency := 1.0
    
    if batchSize >= 32 {
        efficiency = 1.0 + float64(batchSize-32)*0.02 // Scaling efficiency
    } else {
        efficiency = float64(batchSize) / 32.0 // Reduced efficiency for small batches
    }
    
    // Memory bandwidth saturation
    if batchSize > 128 {
        efficiency *= 0.95 // Slight degradation for very large batches
    }
    
    return baseSpeed * efficiency * float64(batchSize) / 32.0
}

func estimateMemoryUsage(batchSize int) float64 {
    // Simplified memory estimation
    // Typical CNN: input + weights + activations + gradients
    baseMemory := 0.5 // GB for batch 32
    return baseMemory * float64(batchSize) / 32.0
}
```

### Dynamic Batch Size Scaling

```go
func demonstrateDynamicBatchScaling() {
    fmt.Println("üîÑ Dynamic Batch Size Scaling")
    fmt.Println("==============================")
    
    fmt.Println("\nüéØ Adaptive Batch Size Strategy:")
    fmt.Println("   1. Start with conservative batch size")
    fmt.Println("   2. Monitor memory usage and performance")
    fmt.Println("   3. Gradually increase until optimal point")
    fmt.Println("   4. Adjust based on model complexity")
    
    fmt.Println("\nüí° Implementation Pattern:")
    fmt.Println(`
func findOptimalBatchSize(model *layers.ModelSpec, maxMemoryGB float32) int {
    batchSizes := []int{16, 32, 64, 128, 256}
    
    for _, batchSize := range batchSizes {
        if estimateMemoryUsage(batchSize) > maxMemoryGB {
            return batchSizes[max(0, i-1)] // Previous size
        }
        
        // Test actual training step
        if !testTrainingStep(model, batchSize) {
            return batchSizes[max(0, i-1)]
        }
    }
    
    return batchSizes[len(batchSizes)-1]
}`)
    
    fmt.Println("\nüìä Batch Size Recommendations by Model Type:")
    
    recommendations := []struct {
        modelType string
        conservative int
        optimal int
        aggressive int
        notes string
    }{
        {"Simple MLP", 64, 128, 256, "Low memory, can use large batches"},
        {"Deep MLP", 32, 64, 128, "More parameters, moderate batches"},
        {"CNN (small)", 32, 64, 128, "Convolutions are memory intensive"},
        {"CNN (large)", 16, 32, 64, "Large feature maps require more memory"},
        {"RNN/LSTM", 16, 32, 64, "Sequential processing limits batch size"},
    }
    
    fmt.Printf("%-12s | %-4s | %-7s | %-10s | %-35s\n",
               "Model Type", "Cons", "Optimal", "Aggressive", "Notes")
    fmt.Println("-------------|------|---------|------------|-----------------------------------")
    
    for _, rec := range recommendations {
        fmt.Printf("%-12s | %-4d | %-7d | %-10d | %-35s\n",
                   rec.modelType, rec.conservative, rec.optimal, 
                   rec.aggressive, rec.notes)
    }
}
```

## üîß Model Architecture Optimization

### Efficient Layer Patterns

```go
func demonstrateEfficientLayerPatterns() {
    fmt.Println("üèóÔ∏è Efficient Layer Patterns for Apple Silicon")
    fmt.Println("=============================================")
    
    fmt.Println("\n‚ö° High-Performance Patterns:")
    
    patterns := []struct {
        pattern string
        description string
        efficiency string
        use_case string
    }{
        {
            "Dense ‚Üí ReLU",
            "Basic MLP building block",
            "Very High",
            "Standard feedforward networks",
        },
        {
            "Conv2D ‚Üí ReLU",
            "CNN feature extraction",
            "High",
            "Image processing",
        },
        {
            "Dense ‚Üí BatchNorm ‚Üí ReLU",
            "Normalized activation",
            "High",
            "Deep networks, stability",
        },
        {
            "Conv2D ‚Üí BatchNorm ‚Üí ReLU",
            "Normalized convolution",
            "High",
            "Deep CNNs",
        },
        {
            "Dense ‚Üí ReLU ‚Üí Dropout",
            "Regularized dense layer",
            "Medium",
            "Preventing overfitting",
        },
    }
    
    fmt.Printf("%-25s | %-25s | %-10s | %-25s\n",
               "Pattern", "Description", "Efficiency", "Use Case")
    fmt.Println("--------------------------|---------------------------|------------|-------------------------")
    
    for _, p := range patterns {
        fmt.Printf("%-25s | %-25s | %-10s | %-25s\n",
                   p.pattern, p.description, p.efficiency, p.use_case)
    }
    
    fmt.Println("\nüöÄ Optimization Tips:")
    fmt.Println("   ‚Ä¢ Prefer ReLU over other activations (fastest)")
    fmt.Println("   ‚Ä¢ Use powers-of-2 layer sizes when possible")
    fmt.Println("   ‚Ä¢ BatchNorm improves convergence speed")
    fmt.Println("   ‚Ä¢ Avoid too many small layers (overhead)")
    
    fmt.Println("\n‚ùå Patterns to Avoid:")
    fmt.Println("   ‚Ä¢ Very small layers (< 32 neurons)")
    fmt.Println("   ‚Ä¢ Excessive layer depth without skip connections")
    fmt.Println("   ‚Ä¢ Mixing precision unnecessarily")
    fmt.Println("   ‚Ä¢ Frequent activation function changes")
}
```

### Memory-Efficient Architectures

```go
func demonstrateMemoryEfficientArchitectures() {
    fmt.Println("üíæ Memory-Efficient Architectures")
    fmt.Println("=================================")
    
    fmt.Println("\nüéØ Memory Optimization Strategies:")
    
    strategies := []struct {
        strategy string
        description string
        memory_saving string
        trade_off string
    }{
        {
            "Progressive Widening",
            "Start narrow, widen gradually",
            "20-30%",
            "Slight accuracy reduction",
        },
        {
            "Depthwise Separable",
            "Separate spatial and channel ops",
            "50-70%",
            "More complex implementation",
        },
        {
            "Grouped Convolutions",
            "Process channels in groups",
            "30-50%",
            "Reduced parameter sharing",
        },
        {
            "Bottleneck Layers",
            "1√ó1 conv for dimension reduction",
            "40-60%",
            "Additional computational steps",
        },
    }
    
    fmt.Printf("%-20s | %-25s | %-13s | %-25s\n",
               "Strategy", "Description", "Memory Saving", "Trade-off")
    fmt.Println("--------------------|---------------------------|---------------|-------------------------")
    
    for _, s := range strategies {
        fmt.Printf("%-20s | %-25s | %-13s | %-25s\n",
                   s.strategy, s.description, s.memory_saving, s.trade_off)
    }
    
    fmt.Println("\nüí° Example: Memory-Efficient CNN")
    fmt.Println(`
    // Traditional CNN (high memory)
    .AddConv2D(64, 3, "conv1")    // 64 filters
    .AddConv2D(128, 3, "conv2")   // 128 filters
    .AddConv2D(256, 3, "conv3")   // 256 filters
    
    // Memory-efficient alternative
    .AddConv2D(32, 3, "conv1")    // Start smaller
    .AddConv2D(64, 3, "conv2")    // Gradual increase
    .AddConv2D(96, 3, "conv3")    // Moderate final size`)
    
    fmt.Println("\nüìä Memory Usage Comparison:")
    fmt.Println("   Traditional:     ~800MB for batch 32")
    fmt.Println("   Memory-optimized: ~400MB for batch 32")
    fmt.Println("   Accuracy loss:    ~2-3% typical")
}
```

## üéõÔ∏è Training Optimization

### Learning Rate Optimization

```go
func demonstrateLearningRateOptimization() {
    fmt.Println("üìà Learning Rate Optimization")
    fmt.Println("==============================")
    
    fmt.Println("\nüéØ Learning Rate Strategies for Apple Silicon:")
    
    strategies := []struct {
        strategy string
        description string
        apple_silicon_benefit string
        implementation string
    }{
        {
            "Linear Warmup",
            "Gradually increase LR from 0",
            "Stable GPU utilization ramp-up",
            "LR = target_lr * (epoch / warmup_epochs)",
        },
        {
            "Cosine Annealing",
            "Smooth cyclic LR schedule",
            "Efficient computation on GPU",
            "LR = min_lr + 0.5*(max_lr-min_lr)*(1+cos(œÄ*epoch/T))",
        },
        {
            "Step Decay",
            "Drop LR at fixed intervals",
            "Simple, GPU-friendly",
            "LR = initial_lr * (decay_rate ^ (epoch // step_size))",
        },
        {
            "Exponential Decay",
            "Continuous LR reduction",
            "Smooth convergence",
            "LR = initial_lr * (decay_rate ^ epoch)",
        },
    }
    
    fmt.Printf("%-17s | %-25s | %-25s | %-35s\n",
               "Strategy", "Description", "Apple Silicon Benefit", "Implementation")
    fmt.Println("------------------|---------------------------|---------------------------|-----------------------------------")
    
    for i, s := range strategies {
        if i > 0 && i%2 == 0 {
            fmt.Println("                  |                           |                           |")
        }
        fmt.Printf("%-17s | %-25s | %-25s | %-35s\n",
                   s.strategy, s.description, s.apple_silicon_benefit, s.implementation)
    }
    
    fmt.Println("\nüí° Apple Silicon Specific Optimizations:")
    fmt.Println("   ‚Ä¢ Use higher initial LR (unified memory allows larger batches)")
    fmt.Println("   ‚Ä¢ Shorter warmup periods (faster GPU initialization)")
    fmt.Println("   ‚Ä¢ More aggressive decay (efficient gradient computation)")
    fmt.Println("   ‚Ä¢ Dynamic adjustment based on GPU utilization")
    
    fmt.Println("\nüìä Recommended Learning Rate Ranges:")
    ranges := []struct {
        model_type string
        initial_lr string
        final_lr string
        schedule string
    }{
        {"Simple MLP", "0.01 - 0.1", "0.001 - 0.01", "Step decay"},
        {"Deep MLP", "0.001 - 0.01", "0.0001 - 0.001", "Cosine annealing"},
        {"CNN (small)", "0.001 - 0.01", "0.0001 - 0.001", "Step decay + warmup"},
        {"CNN (large)", "0.0001 - 0.001", "0.00001 - 0.0001", "Cosine + warmup"},
    }
    
    fmt.Printf("%-12s | %-12s | %-12s | %-20s\n",
               "Model Type", "Initial LR", "Final LR", "Recommended Schedule")
    fmt.Println("-------------|--------------|--------------|--------------------")
    
    for _, r := range ranges {
        fmt.Printf("%-12s | %-12s | %-12s | %-20s\n",
                   r.model_type, r.initial_lr, r.final_lr, r.schedule)
    }
}
```

### Optimizer Selection for Performance

```go
func demonstrateOptimizerPerformance() {
    fmt.Println("‚ö° Optimizer Performance on Apple Silicon")
    fmt.Println("========================================")
    
    fmt.Println("\nüöÄ Performance Characteristics:")
    
    optimizers := []struct {
        optimizer string
        gpu_efficiency string
        memory_usage string
        convergence_speed string
        best_for string
    }{
        {
            "Adam",
            "Very High",
            "High",
            "Fast",
            "Most deep learning tasks",
        },
        {
            "SGD",
            "Highest",
            "Lowest",
            "Medium",
            "Simple models, fine-tuning",
        },
        {
            "RMSProp",
            "High",
            "Medium",
            "Fast",
            "RNNs, non-stationary objectives",
        },
        {
            "AdaGrad",
            "High",
            "Medium",
            "Fast‚ÜíSlow",
            "Sparse features",
        },
        {
            "Nadam",
            "High",
            "High",
            "Very Fast",
            "When you need best convergence",
        },
    }
    
    fmt.Printf("%-10s | %-13s | %-12s | %-17s | %-25s\n",
               "Optimizer", "GPU Efficiency", "Memory Usage", "Convergence Speed", "Best For")
    fmt.Println("-----------|---------------|--------------|-------------------|-------------------------")
    
    for _, opt := range optimizers {
        fmt.Printf("%-10s | %-13s | %-12s | %-17s | %-25s\n",
                   opt.optimizer, opt.gpu_efficiency, opt.memory_usage, 
                   opt.convergence_speed, opt.best_for)
    }
    
    fmt.Println("\nüéØ Apple Silicon Optimizer Recommendations:")
    fmt.Println("   ‚Ä¢ Adam: Default choice, excellent GPU utilization")
    fmt.Println("   ‚Ä¢ SGD: When memory is extremely limited")
    fmt.Println("   ‚Ä¢ Nadam: When training time is critical")
    fmt.Println("   ‚Ä¢ RMSProp: For recurrent architectures")
    
    fmt.Println("\n‚öôÔ∏è Optimizer Configuration for Performance:")
    fmt.Println(`
    // High-performance Adam configuration
    config := training.TrainerConfig{
        OptimizerType: cgo_bridge.Adam,
        LearningRate:  0.002,    // Slightly higher for Apple Silicon
        Beta1:         0.9,      // Standard momentum
        Beta2:         0.999,    // Standard adaptive term
        Epsilon:       1e-8,     // Numerical stability
    }`)
}
```

## üîÑ Data Pipeline Optimization

### Efficient Data Loading

```go
func demonstrateEfficientDataLoading() {
    fmt.Println("üì• Efficient Data Loading for Apple Silicon")
    fmt.Println("==========================================")
    
    fmt.Println("\nüéØ Data Loading Optimization Strategies:")
    
    strategies := []struct {
        strategy string
        description string
        performance_gain string
        implementation_effort string
    }{
        {
            "Batch Preprocessing",
            "Process multiple samples together",
            "2-3x speedup",
            "Low",
        },
        {
            "Memory Mapping",
            "Map data files directly to memory",
            "3-5x speedup",
            "Medium",
        },
        {
            "Async Loading",
            "Load next batch while training current",
            "10-20% speedup",
            "Medium",
        },
        {
            "Data Prefetching",
            "GPU-resident data preparation",
            "15-25% speedup",
            "High",
        },
    }
    
    fmt.Printf("%-18s | %-30s | %-15s | %-20s\n",
               "Strategy", "Description", "Performance Gain", "Implementation Effort")
    fmt.Println("-------------------|--------------------------------|-----------------|--------------------")
    
    for _, s := range strategies {
        fmt.Printf("%-18s | %-30s | %-15s | %-20s\n",
                   s.strategy, s.description, s.performance_gain, s.implementation_effort)
    }
    
    fmt.Println("\nüí° Implementation Pattern:")
    fmt.Println(`
    // Efficient batch loading
    func loadBatchEfficiently(batchSize int) ([]float32, []int32) {
        // 1. Pre-allocate buffers
        inputBuffer := make([]float32, batchSize*inputSize)
        labelBuffer := make([]int32, batchSize)
        
        // 2. Batch read operations
        for i := 0; i < batchSize; i++ {
            // Load directly into buffer
            loadSampleIntoBuffer(inputBuffer, i*inputSize, i)
            labelBuffer[i] = loadLabel(i)
        }
        
        return inputBuffer, labelBuffer
    }`)
    
    fmt.Println("\nüöÄ Apple Silicon Specific Optimizations:")
    fmt.Println("   ‚Ä¢ Use unified memory for zero-copy transfers")
    fmt.Println("   ‚Ä¢ Leverage high memory bandwidth")
    fmt.Println("   ‚Ä¢ Minimize CPU-GPU synchronization")
    fmt.Println("   ‚Ä¢ Use native data formats when possible")
}
```

### Memory Management Best Practices

```go
func demonstrateMemoryManagementOptimization() {
    fmt.Println("üíæ Memory Management Optimization")
    fmt.Println("=================================")
    
    fmt.Println("\nüéØ Memory Optimization Techniques:")
    
    techniques := []struct {
        technique string
        description string
        memory_saving string
        performance_impact string
    }{
        {
            "Buffer Reuse",
            "Reuse allocated GPU buffers",
            "30-50%",
            "10-15% speedup",
        },
        {
            "Memory Pooling",
            "Pre-allocate common buffer sizes",
            "20-30%",
            "5-10% speedup",
        },
        {
            "Lazy Allocation",
            "Allocate memory only when needed",
            "Variable",
            "Reduced startup time",
        },
        {
            "Gradient Checkpointing",
            "Trade computation for memory",
            "50-80%",
            "20-30% slowdown",
        },
    }
    
    fmt.Printf("%-20s | %-30s | %-13s | %-18s\n",
               "Technique", "Description", "Memory Saving", "Performance Impact")
    fmt.Println("--------------------|--------------------------------|---------------|------------------")
    
    for _, t := range techniques {
        fmt.Printf("%-20s | %-30s | %-13s | %-18s\n",
                   t.technique, t.description, t.memory_saving, t.performance_impact)
    }
    
    fmt.Println("\nüîß Go-Metal Memory Management:")
    fmt.Println("   ‚Ä¢ Automatic buffer pooling")
    fmt.Println("   ‚Ä¢ Reference counting for cleanup")
    fmt.Println("   ‚Ä¢ Unified memory optimization")
    fmt.Println("   ‚Ä¢ GPU-resident computation")
    
    fmt.Println("\nüí° Best Practices:")
    fmt.Println("   ‚Ä¢ Always call defer trainer.Cleanup()")
    fmt.Println("   ‚Ä¢ Use appropriate batch sizes")
    fmt.Println("   ‚Ä¢ Monitor memory usage during development")
    fmt.Println("   ‚Ä¢ Release unused tensors promptly")
    
    fmt.Println("\n‚ö†Ô∏è Memory Anti-patterns:")
    fmt.Println("   ‚Ä¢ Frequent small allocations")
    fmt.Println("   ‚Ä¢ Memory leaks from unreleased tensors")
    fmt.Println("   ‚Ä¢ Unnecessary CPU-GPU transfers")
    fmt.Println("   ‚Ä¢ Over-sized intermediate buffers")
}
```

## üìä Performance Profiling and Monitoring

### Built-in Performance Monitoring

```go
func demonstratePerformanceMonitoring() {
    fmt.Println("üìà Performance Monitoring and Profiling")
    fmt.Println("=======================================")
    
    fmt.Println("\nüîç Key Performance Metrics:")
    
    metrics := []struct {
        metric string
        description string
        good_value string
        monitoring_method string
    }{
        {
            "Samples/Second",
            "Training throughput",
            "> 1000 for MLPs, > 100 for CNNs",
            "Built-in timing",
        },
        {
            "GPU Utilization",
            "Percentage of GPU compute used",
            "> 80%",
            "Activity Monitor, powermetrics",
        },
        {
            "Memory Efficiency",
            "GPU memory usage vs allocation",
            "< 90% of available",
            "Memory profiler",
        },
        {
            "Batch Processing Time",
            "Time per training step",
            "< 100ms for real-time",
            "time.Since() measurements",
        },
    }
    
    fmt.Printf("%-20s | %-25s | %-25s | %-20s\n",
               "Metric", "Description", "Good Value", "Monitoring Method")
    fmt.Println("---------------------|---------------------------|---------------------------|--------------------")
    
    for _, m := range metrics {
        fmt.Printf("%-20s | %-25s | %-25s | %-20s\n",
                   m.metric, m.description, m.good_value, m.monitoring_method)
    }
    
    fmt.Println("\nüí° Performance Monitoring Code:")
    fmt.Println(`
    func monitorTrainingPerformance(trainer *training.ModelTrainer) {
        startTime := time.Now()
        samplesProcessed := 0
        
        for epoch := 1; epoch <= maxEpochs; epoch++ {
            batchStart := time.Now()
            
            result, err := trainer.TrainBatch(inputs, inputShape, labels, labelShape)
            if err != nil {
                log.Printf("Training error: %v", err)
                continue
            }
            
            batchTime := time.Since(batchStart)
            samplesProcessed += batchSize
            
            // Calculate metrics
            samplesPerSec := float64(batchSize) / batchTime.Seconds()
            totalTime := time.Since(startTime)
            avgSamplesPerSec := float64(samplesProcessed) / totalTime.Seconds()
            
            if epoch%10 == 0 {
                log.Printf("Epoch %d: %.1f samples/sec (avg: %.1f), loss: %.4f",
                          epoch, samplesPerSec, avgSamplesPerSec, result.Loss)
            }
        }
    }`)
}
```

### Bottleneck Identification

```go
func demonstrateBottleneckIdentification() {
    fmt.Println("üîç Performance Bottleneck Identification")
    fmt.Println("========================================")
    
    fmt.Println("\nüéØ Common Bottlenecks and Solutions:")
    
    bottlenecks := []struct {
        bottleneck string
        symptoms string
        solutions string
        tools string
    }{
        {
            "CPU Data Loading",
            "Low GPU utilization, high CPU usage",
            "Async loading, batch preprocessing",
            "Activity Monitor, htop",
        },
        {
            "Memory Bandwidth",
            "Low throughput with high GPU util",
            "Optimize data layouts, reduce transfers",
            "Instruments, Memory Graph",
        },
        {
            "Small Batch Sizes",
            "GPU underutilized, poor scaling",
            "Increase batch size, gradient accumulation",
            "Built-in profiling",
        },
        {
            "Inefficient Model",
            "High memory, low computation",
            "Architecture optimization, pruning",
            "Model analysis tools",
        },
    }
    
    fmt.Printf("%-18s | %-25s | %-25s | %-20s\n",
               "Bottleneck", "Symptoms", "Solutions", "Diagnostic Tools")
    fmt.Println("-------------------|---------------------------|---------------------------|--------------------")
    
    for _, b := range bottlenecks {
        fmt.Printf("%-18s | %-25s | %-25s | %-20s\n",
                   b.bottleneck, b.symptoms, b.solutions, b.tools)
    }
    
    fmt.Println("\nüîß Diagnostic Process:")
    fmt.Println("   1. Measure baseline performance")
    fmt.Println("   2. Profile GPU and CPU utilization")
    fmt.Println("   3. Identify the limiting factor")
    fmt.Println("   4. Apply targeted optimizations")
    fmt.Println("   5. Measure improvement and iterate")
    
    fmt.Println("\nüí° Quick Diagnostic Commands:")
    fmt.Println("   # Overall system performance")
    fmt.Println("   top -pid $(pgrep -f your_program)")
    fmt.Println("   ")
    fmt.Println("   # GPU utilization")
    fmt.Println("   sudo powermetrics --samplers gpu_power -a --buffer-size=1")
    fmt.Println("   ")
    fmt.Println("   # Memory pressure")
    fmt.Println("   memory_pressure && vm_stat")
}
```

## üöÄ Advanced Optimization Techniques

### Gradient Accumulation for Large Effective Batch Sizes

```go
func demonstrateGradientAccumulation() {
    fmt.Println("üîÑ Gradient Accumulation for Large Effective Batch Sizes")
    fmt.Println("========================================================")
    
    fmt.Println("\nüéØ When to Use Gradient Accumulation:")
    fmt.Println("   ‚Ä¢ Want large effective batch size but limited memory")
    fmt.Println("   ‚Ä¢ Training very large models")
    fmt.Println("   ‚Ä¢ Maintaining gradient stability")
    fmt.Println("   ‚Ä¢ Simulating multi-GPU training on single device")
    
    fmt.Println("\nüí° Concept:")
    fmt.Println("   Instead of:")
    fmt.Println("     ‚ùå Process 256 samples at once (may not fit in memory)")
    fmt.Println("   ")
    fmt.Println("   Do this:")
    fmt.Println("     ‚úÖ Process 4 batches of 64 samples each")
    fmt.Println("     ‚úÖ Accumulate gradients across batches")
    fmt.Println("     ‚úÖ Update parameters once after all 4 batches")
    
    fmt.Println("\nüìä Benefits vs Trade-offs:")
    
    tradeoffs := []struct {
        aspect string
        benefit string
        tradeoff string
    }{
        {"Memory Usage", "Reduced peak memory", "More computation steps"},
        {"Gradient Quality", "Large effective batch stability", "Delayed parameter updates"},
        {"Training Speed", "Better GPU utilization", "Slight overhead per step"},
        {"Model Quality", "Improved convergence", "More complex implementation"},
    }
    
    fmt.Printf("%-15s | %-25s | %-25s\n", "Aspect", "Benefit", "Trade-off")
    fmt.Println("----------------|---------------------------|---------------------------")
    
    for _, t := range tradeoffs {
        fmt.Printf("%-15s | %-25s | %-25s\n", t.aspect, t.benefit, t.tradeoff)
    }
    
    fmt.Println("\n‚ö° Implementation Strategy:")
    fmt.Println(`
    // Conceptual gradient accumulation pattern
    func trainWithGradientAccumulation(trainer *training.ModelTrainer, 
                                      data []float32, labels []int32,
                                      physicalBatch, effectiveBatch int) {
        
        accumulationSteps := effectiveBatch / physicalBatch
        
        for step := 0; step < accumulationSteps; step++ {
            batchStart := step * physicalBatch
            batchEnd := (step + 1) * physicalBatch
            
            batchData := data[batchStart*inputSize : batchEnd*inputSize]
            batchLabels := labels[batchStart : batchEnd]
            
            // Train but don't update parameters yet (conceptual)
            result, _ := trainer.TrainBatch(batchData, inputShape, 
                                          batchLabels, labelShape)
            
            // In actual implementation, gradients would accumulate
        }
        
        // Parameters updated once after all accumulation steps
    }`)
}
```

### Model Parallelism Strategies

```go
func demonstrateModelParallelism() {
    fmt.Println("üîÄ Model Parallelism Strategies")
    fmt.Println("===============================")
    
    fmt.Println("\nüéØ Types of Parallelism:")
    
    parallelism := []struct {
        type_name string
        description string
        apple_silicon_fit string
        complexity string
    }{
        {
            "Data Parallelism",
            "Same model, different data batches",
            "Good (unified memory)",
            "Low",
        },
        {
            "Layer Parallelism",
            "Different layers on different cores",
            "Excellent (heterogeneous)",
            "Medium",
        },
        {
            "Pipeline Parallelism",
            "Pipeline stages across compute units",
            "Very Good (CPU+GPU+ANE)",
            "High",
        },
        {
            "Tensor Parallelism",
            "Split large tensors across units",
            "Limited (single GPU)",
            "Very High",
        },
    }
    
    fmt.Printf("%-18s | %-30s | %-20s | %-10s\n",
               "Type", "Description", "Apple Silicon Fit", "Complexity")
    fmt.Println("-------------------|--------------------------------|----------------------|----------")
    
    for _, p := range parallelism {
        fmt.Printf("%-18s | %-30s | %-20s | %-10s\n",
                   p.type_name, p.description, p.apple_silicon_fit, p.complexity)
    }
    
    fmt.Println("\nüöÄ Apple Silicon Advantages:")
    fmt.Println("   ‚Ä¢ Unified memory enables efficient data sharing")
    fmt.Println("   ‚Ä¢ Heterogeneous compute (CPU, GPU, Neural Engine)")
    fmt.Println("   ‚Ä¢ High memory bandwidth")
    fmt.Println("   ‚Ä¢ Low-latency inter-unit communication")
    
    fmt.Println("\nüí° Practical Implementation:")
    fmt.Println("   ‚Ä¢ Use CPU for data preprocessing")
    fmt.Println("   ‚Ä¢ GPU for main computation")
    fmt.Println("   ‚Ä¢ Neural Engine for specific operations (when available)")
    fmt.Println("   ‚Ä¢ Overlap computation and data movement")
}
```

## üìà Performance Benchmarking

### Comprehensive Benchmarking Suite

```go
func runPerformanceBenchmarks() {
    fmt.Println("üèÅ Comprehensive Performance Benchmarking")
    fmt.Println("=========================================")
    
    // Initialize Metal device and memory manager
    device, err := cgo_bridge.CreateMetalDevice()
    if err != nil {
        log.Fatalf("Failed to create Metal device: %v", err)
    }
    defer cgo_bridge.DestroyMetalDevice(device)
    
    memory.InitializeGlobalMemoryManager(device)
    
    fmt.Println("\nüéØ Benchmark Suite:")
    
    benchmarks := []struct {
        name string
        description string
        config BenchmarkConfig
    }{
        {
            "MLP Small",
            "Simple feedforward network",
            BenchmarkConfig{ModelType: "MLP", Size: "Small", BatchSize: 64},
        },
        {
            "MLP Large",
            "Deep feedforward network",
            BenchmarkConfig{ModelType: "MLP", Size: "Large", BatchSize: 32},
        },
        {
            "CNN Small",
            "Basic convolutional network",
            BenchmarkConfig{ModelType: "CNN", Size: "Small", BatchSize: 32},
        },
        {
            "CNN Large",
            "Deep convolutional network",
            BenchmarkConfig{ModelType: "CNN", Size: "Large", BatchSize: 16},
        },
    }
    
    fmt.Printf("%-10s | %-25s | %-15s | %-12s | %-15s\n",
               "Benchmark", "Description", "Samples/Sec", "Memory (GB)", "GPU Util (%)")
    fmt.Println("-----------|---------------------------|-----------------|--------------|---------------")
    
    for _, bench := range benchmarks {
        result := runSingleBenchmark(bench.config)
        fmt.Printf("%-10s | %-25s | %-15.1f | %-12.2f | %-15.1f\n",
                   bench.name, bench.description, result.SamplesPerSec,
                   result.MemoryUsageGB, result.GPUUtilization)
    }
    
    fmt.Println("\nüìä Benchmark Analysis:")
    fmt.Println("   ‚Ä¢ MLPs generally achieve higher throughput")
    fmt.Println("   ‚Ä¢ CNNs are more memory intensive")
    fmt.Println("   ‚Ä¢ Larger batch sizes improve efficiency")
    fmt.Println("   ‚Ä¢ Apple Silicon excels at unified workloads")
}

type BenchmarkConfig struct {
    ModelType string
    Size      string
    BatchSize int
}

type BenchmarkResult struct {
    SamplesPerSec   float64
    MemoryUsageGB   float64
    GPUUtilization  float64
}

func runSingleBenchmark(config BenchmarkConfig) BenchmarkResult {
    // Simplified benchmark implementation
    // Real implementation would create actual models and measure performance
    
    baseSpeed := 1000.0
    baseMemory := 1.0
    baseGPU := 85.0
    
    // Adjust based on model type and size
    if config.ModelType == "CNN" {
        baseSpeed *= 0.3  // CNNs are slower
        baseMemory *= 2.0 // CNNs use more memory
    }
    
    if config.Size == "Large" {
        baseSpeed *= 0.5  // Larger models are slower
        baseMemory *= 3.0 // Larger models use more memory
    }
    
    // Batch size scaling
    batchScaling := float64(config.BatchSize) / 32.0
    baseSpeed *= batchScaling
    baseMemory *= batchScaling
    
    return BenchmarkResult{
        SamplesPerSec:  baseSpeed,
        MemoryUsageGB:  baseMemory,
        GPUUtilization: baseGPU,
    }
}
```

## üéØ Performance Best Practices Summary

### Quick Optimization Checklist

```go
func performanceOptimizationChecklist() {
    fmt.Println("‚úÖ Performance Optimization Checklist")
    fmt.Println("=====================================")
    
    fmt.Println("\nüèóÔ∏è Model Architecture:")
    fmt.Println("   ‚òê Use efficient layer patterns (Dense‚ÜíReLU, Conv2D‚ÜíReLU)")
    fmt.Println("   ‚òê Prefer ReLU activations for speed")
    fmt.Println("   ‚òê Use power-of-2 layer sizes when possible")
    fmt.Println("   ‚òê Avoid excessively deep networks without skip connections")
    
    fmt.Println("\nüì¶ Batch Configuration:")
    fmt.Println("   ‚òê Use largest batch size that fits in memory")
    fmt.Println("   ‚òê Start with 32-64, increase gradually")
    fmt.Println("   ‚òê Consider gradient accumulation for larger effective batches")
    fmt.Println("   ‚òê Monitor GPU utilization vs batch size")
    
    fmt.Println("\n‚öôÔ∏è Training Setup:")
    fmt.Println("   ‚òê Choose Adam for most cases, SGD for simple models")
    fmt.Println("   ‚òê Use appropriate learning rates (0.001 for Adam)")
    fmt.Println("   ‚òê Implement learning rate scheduling")
    fmt.Println("   ‚òê Monitor convergence and adjust hyperparameters")
    
    fmt.Println("\nüíæ Memory Management:")
    fmt.Println("   ‚òê Always call defer trainer.Cleanup()")
    fmt.Println("   ‚òê Monitor memory usage during development")
    fmt.Println("   ‚òê Use GPU-resident data when possible")
    fmt.Println("   ‚òê Avoid frequent small allocations")
    
    fmt.Println("\nüìä Monitoring & Profiling:")
    fmt.Println("   ‚òê Track samples/second and loss progression")
    fmt.Println("   ‚òê Monitor GPU utilization (target >80%)")
    fmt.Println("   ‚òê Profile memory usage and identify bottlenecks")
    fmt.Println("   ‚òê Use Activity Monitor and powermetrics for system monitoring")
    
    fmt.Println("\nüöÄ Apple Silicon Specific:")
    fmt.Println("   ‚òê Leverage unified memory architecture")
    fmt.Println("   ‚òê Use larger batch sizes than traditional GPUs")
    fmt.Println("   ‚òê Minimize CPU-GPU synchronization")
    fmt.Println("   ‚òê Take advantage of high memory bandwidth")
}
```

## üöÄ Next Steps

Master performance optimization in go-metal:

- **[Mixed Precision Tutorial](../tutorials/mixed-precision.md)** - 86% speedup with FP16 training
- **[Visualization Guide](visualization.md)** - Monitor performance with real-time plots
- **[Advanced Examples](../examples/)** - See optimizations in production applications

**Ready for production?** Apply these optimizations to achieve maximum performance on Apple Silicon with go-metal.

---

## üß† Key Takeaways

- **Apple Silicon advantages**: Unified memory, high bandwidth, efficient GPU utilization
- **Batch size optimization**: Start with 32-64, increase until memory limits
- **Architecture efficiency**: Use proven patterns, avoid unnecessary complexity
- **Monitoring is crucial**: Track GPU utilization, memory usage, and throughput
- **Iterative optimization**: Profile, optimize, measure, repeat

With these performance optimization techniques, you can achieve maximum training speed and efficiency on Apple Silicon!