
<!DOCTYPE html>
<html>
	<head>
		<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
		<title>optimizer: Go Coverage Report</title>
		<style>
			body {
				background: black;
				color: rgb(80, 80, 80);
			}
			body, pre, #legend span {
				font-family: Menlo, monospace;
				font-weight: bold;
			}
			#topbar {
				background: black;
				position: fixed;
				top: 0; left: 0; right: 0;
				height: 42px;
				border-bottom: 1px solid rgb(80, 80, 80);
			}
			#content {
				margin-top: 50px;
			}
			#nav, #legend {
				float: left;
				margin-left: 10px;
			}
			#legend {
				margin-top: 12px;
			}
			#nav {
				margin-top: 10px;
			}
			#legend span {
				margin: 0 5px;
			}
			.cov0 { color: rgb(192, 0, 0) }
.cov1 { color: rgb(128, 128, 128) }
.cov2 { color: rgb(116, 140, 131) }
.cov3 { color: rgb(104, 152, 134) }
.cov4 { color: rgb(92, 164, 137) }
.cov5 { color: rgb(80, 176, 140) }
.cov6 { color: rgb(68, 188, 143) }
.cov7 { color: rgb(56, 200, 146) }
.cov8 { color: rgb(44, 212, 149) }
.cov9 { color: rgb(32, 224, 152) }
.cov10 { color: rgb(20, 236, 155) }

		</style>
	</head>
	<body>
		<div id="topbar">
			<div id="nav">
				<select id="files">
				
				<option value="file0">github.com/tsawler/go-metal/optimizer/adadelta.go (1.8%)</option>
				
				<option value="file1">github.com/tsawler/go-metal/optimizer/adagrad.go (0.0%)</option>
				
				<option value="file2">github.com/tsawler/go-metal/optimizer/adam.go (13.8%)</option>
				
				<option value="file3">github.com/tsawler/go-metal/optimizer/lbfgs.go (0.0%)</option>
				
				<option value="file4">github.com/tsawler/go-metal/optimizer/nadam.go (1.2%)</option>
				
				<option value="file5">github.com/tsawler/go-metal/optimizer/optimizer_test_fixed.go (0.0%)</option>
				
				<option value="file6">github.com/tsawler/go-metal/optimizer/rmsprop.go (0.9%)</option>
				
				</select>
			</div>
			<div id="legend">
				<span>not tracked</span>
			
				<span class="cov0">not covered</span>
				<span class="cov8">covered</span>
			
			</div>
		</div>
		<div id="content">
		
		<pre class="file" id="file0" style="display: none">package optimizer

import (
        "fmt"
        "unsafe"

        "github.com/tsawler/go-metal/cgo_bridge"
        "github.com/tsawler/go-metal/memory"
)

// AdaDeltaOptimizerState represents GPU-resident AdaDelta optimizer state
type AdaDeltaOptimizerState struct {
        // Configuration
        config AdaDeltaConfig
        
        // GPU-resident state buffers
        squaredGradAvgBuffers []unsafe.Pointer // E[g^2]_t - Accumulated squared gradient averages
        squaredUpdateAvgBuffers []unsafe.Pointer // E[Δx^2]_t - Accumulated squared update averages
        WeightBuffers         []unsafe.Pointer // Current weight tensors
        
        // Step tracking
        currentStep uint64
        
        // Buffer management
        memoryManager *memory.MemoryManager
        device        unsafe.Pointer
        bufferSizes   []int
        
        // Command buffer pooling
        commandPool unsafe.Pointer
        usePooling  bool
}

// AdaDeltaConfig holds configuration for AdaDelta optimizer
type AdaDeltaConfig struct {
        Rho         float32 // Decay rate for moving averages (typically 0.95)
        Epsilon     float32 // Small constant for numerical stability
        WeightDecay float32 // L2 regularization strength
}

// DefaultAdaDeltaConfig returns default AdaDelta optimizer configuration
func DefaultAdaDeltaConfig() AdaDeltaConfig <span class="cov8" title="1">{
        return AdaDeltaConfig{
                Rho:         0.95,
                Epsilon:     1e-6,
                WeightDecay: 0.0,
        }
}</span>

// NewAdaDeltaOptimizer creates a new GPU-resident AdaDelta optimizer
func NewAdaDeltaOptimizer(
        config AdaDeltaConfig,
        weightShapes [][]int,
        memoryManager *memory.MemoryManager,
        device unsafe.Pointer,
) (*AdaDeltaOptimizerState, error) <span class="cov0" title="0">{
        if memoryManager == nil </span><span class="cov0" title="0">{
                return nil, fmt.Errorf("memory manager cannot be nil")
        }</span>
        
        <span class="cov0" title="0">if device == nil </span><span class="cov0" title="0">{
                return nil, fmt.Errorf("device cannot be nil")
        }</span>
        
        <span class="cov0" title="0">if len(weightShapes) == 0 </span><span class="cov0" title="0">{
                return nil, fmt.Errorf("no weight shapes provided")
        }</span>
        
        <span class="cov0" title="0">if config.Rho &lt;= 0 || config.Rho &gt;= 1 </span><span class="cov0" title="0">{
                return nil, fmt.Errorf("rho must be in range (0, 1), got %f", config.Rho)
        }</span>
        
        <span class="cov0" title="0">numWeights := len(weightShapes)
        
        adadelta := &amp;AdaDeltaOptimizerState{
                config:                  config,
                squaredGradAvgBuffers:   make([]unsafe.Pointer, numWeights),
                squaredUpdateAvgBuffers: make([]unsafe.Pointer, numWeights),
                WeightBuffers:           make([]unsafe.Pointer, numWeights),
                currentStep:             0,
                memoryManager:           memoryManager,
                device:                  device,
                bufferSizes:             make([]int, numWeights),
        }
        
        // Calculate buffer sizes and allocate state buffers
        for i, shape := range weightShapes </span><span class="cov0" title="0">{
                adadelta.bufferSizes[i] = calculateTensorSize(shape) * 4 // 4 bytes per float32
                
                // Allocate squared gradient average buffer (E[g^2])
                squaredGradAvgBuffer := adadelta.memoryManager.AllocateBuffer(adadelta.bufferSizes[i])
                if squaredGradAvgBuffer == nil </span><span class="cov0" title="0">{
                        adadelta.cleanup()
                        return nil, fmt.Errorf("failed to allocate squared gradient average buffer for weight %d", i)
                }</span>
                <span class="cov0" title="0">adadelta.squaredGradAvgBuffers[i] = squaredGradAvgBuffer
                
                // Allocate squared update average buffer (E[Δx^2])
                squaredUpdateAvgBuffer := adadelta.memoryManager.AllocateBuffer(adadelta.bufferSizes[i])
                if squaredUpdateAvgBuffer == nil </span><span class="cov0" title="0">{
                        adadelta.cleanup()
                        return nil, fmt.Errorf("failed to allocate squared update average buffer for weight %d", i)
                }</span>
                <span class="cov0" title="0">adadelta.squaredUpdateAvgBuffers[i] = squaredUpdateAvgBuffer
                
                // Initialize both to zero
                if err := cgo_bridge.ZeroMetalBuffer(adadelta.device, squaredGradAvgBuffer, adadelta.bufferSizes[i]); err != nil </span><span class="cov0" title="0">{
                        adadelta.cleanup()
                        return nil, fmt.Errorf("failed to zero squared gradient average buffer: %v", err)
                }</span>
                <span class="cov0" title="0">if err := cgo_bridge.ZeroMetalBuffer(adadelta.device, squaredUpdateAvgBuffer, adadelta.bufferSizes[i]); err != nil </span><span class="cov0" title="0">{
                        adadelta.cleanup()
                        return nil, fmt.Errorf("failed to zero squared update average buffer: %v", err)
                }</span>
        }
        
        <span class="cov0" title="0">return adadelta, nil</span>
}

// cleanup releases all allocated buffers
func (adadelta *AdaDeltaOptimizerState) cleanup() <span class="cov0" title="0">{
        for i := range adadelta.squaredGradAvgBuffers </span><span class="cov0" title="0">{
                if adadelta.squaredGradAvgBuffers[i] != nil </span><span class="cov0" title="0">{
                        adadelta.memoryManager.ReleaseBuffer(adadelta.squaredGradAvgBuffers[i])
                }</span>
                <span class="cov0" title="0">if adadelta.squaredUpdateAvgBuffers[i] != nil </span><span class="cov0" title="0">{
                        adadelta.memoryManager.ReleaseBuffer(adadelta.squaredUpdateAvgBuffers[i])
                }</span>
        }
}

// SetWeightBuffers sets the current weight buffer pointers
func (adadelta *AdaDeltaOptimizerState) SetWeightBuffers(weightBuffers []unsafe.Pointer) error <span class="cov0" title="0">{
        if len(weightBuffers) != len(adadelta.WeightBuffers) </span><span class="cov0" title="0">{
                return fmt.Errorf("expected %d weight buffers, got %d", len(adadelta.WeightBuffers), len(weightBuffers))
        }</span>
        
        <span class="cov0" title="0">copy(adadelta.WeightBuffers, weightBuffers)
        return nil</span>
}

// Step performs a single AdaDelta optimization step
func (adadelta *AdaDeltaOptimizerState) Step(gradientBuffers []unsafe.Pointer) error <span class="cov0" title="0">{
        if len(gradientBuffers) != len(adadelta.WeightBuffers) </span><span class="cov0" title="0">{
                return fmt.Errorf("gradient buffers length (%d) doesn't match weight buffers length (%d)",
                        len(gradientBuffers), len(adadelta.WeightBuffers))
        }</span>
        
        <span class="cov0" title="0">adadelta.currentStep++
        
        // Execute AdaDelta step using CGO bridge
        var err error
        if adadelta.usePooling &amp;&amp; adadelta.commandPool != nil </span><span class="cov0" title="0">{
                err = cgo_bridge.ExecuteAdaDeltaStepMPSGraphPooled(
                        adadelta.device,
                        adadelta.WeightBuffers,
                        gradientBuffers,
                        adadelta.squaredGradAvgBuffers,
                        adadelta.squaredUpdateAvgBuffers,
                        len(adadelta.WeightBuffers),
                        adadelta.bufferSizes,
                        adadelta.config.Rho,
                        adadelta.config.Epsilon,
                        adadelta.config.WeightDecay,
                        adadelta.commandPool,
                )
        }</span> else<span class="cov0" title="0"> {
                err = cgo_bridge.ExecuteAdaDeltaStepMPSGraph(
                        adadelta.device,
                        adadelta.WeightBuffers,
                        gradientBuffers,
                        adadelta.squaredGradAvgBuffers,
                        adadelta.squaredUpdateAvgBuffers,
                        len(adadelta.WeightBuffers),
                        adadelta.bufferSizes,
                        adadelta.config.Rho,
                        adadelta.config.Epsilon,
                        adadelta.config.WeightDecay,
                )
        }</span>
        
        <span class="cov0" title="0">if err != nil </span><span class="cov0" title="0">{
                return fmt.Errorf("AdaDelta step failed: %v", err)
        }</span>
        
        // Log progress periodically
        <span class="cov0" title="0">if adadelta.currentStep%10 == 0 </span><span class="cov0" title="0">{
                fmt.Printf("AdaDelta step %d completed, rho=%.3f\n", adadelta.currentStep, adadelta.config.Rho)
        }</span>
        
        <span class="cov0" title="0">return nil</span>
}

// SetCommandPool sets the command buffer pool for Metal operations
func (adadelta *AdaDeltaOptimizerState) SetCommandPool(pool unsafe.Pointer) <span class="cov0" title="0">{
        adadelta.commandPool = pool
        adadelta.usePooling = (pool != nil)
}</span>

// GetStep returns the current optimization step count
func (adadelta *AdaDeltaOptimizerState) GetStep() uint64 <span class="cov0" title="0">{
        return adadelta.currentStep
}</span>

// GetStats returns optimizer statistics
func (adadelta *AdaDeltaOptimizerState) GetStats() map[string]interface{} <span class="cov0" title="0">{
        return map[string]interface{}{
                "step":        adadelta.currentStep,
                "rho":         adadelta.config.Rho,
                "epsilon":     adadelta.config.Epsilon,
                "weight_decay": adadelta.config.WeightDecay,
        }
}</span>

// Cleanup releases all GPU buffers
func (adadelta *AdaDeltaOptimizerState) Cleanup() <span class="cov0" title="0">{
        adadelta.cleanup()
}</span>

// UpdateLearningRate is not used in AdaDelta (it adapts automatically)
func (adadelta *AdaDeltaOptimizerState) UpdateLearningRate(newLR float32) error <span class="cov0" title="0">{
        return fmt.Errorf("AdaDelta does not use a fixed learning rate; it adapts automatically based on parameter updates")
}</pre>
		
		<pre class="file" id="file1" style="display: none">package optimizer

import (
        "fmt"
        "unsafe"

        "github.com/tsawler/go-metal/cgo_bridge"
        "github.com/tsawler/go-metal/memory"
)

// AdaGradOptimizerState represents GPU-resident AdaGrad optimizer state
type AdaGradOptimizerState struct {
        // Configuration
        config AdaGradConfig
        
        // GPU-resident state buffers
        squaredGradAvgBuffers []unsafe.Pointer // Accumulated squared gradient averages
        WeightBuffers         []unsafe.Pointer // Current weight tensors
        
        // Step tracking
        currentStep uint64
        
        // Buffer management
        memoryManager *memory.MemoryManager
        device        unsafe.Pointer
        bufferSizes   []int
        
        // Command buffer pooling
        commandPool unsafe.Pointer
        usePooling  bool
}

// AdaGradConfig holds configuration for AdaGrad optimizer
type AdaGradConfig struct {
        LearningRate float32 // Learning rate
        Epsilon      float32 // Small constant for numerical stability
        WeightDecay  float32 // L2 regularization strength
}

// DefaultAdaGradConfig returns default AdaGrad optimizer configuration
func DefaultAdaGradConfig() AdaGradConfig <span class="cov0" title="0">{
        return AdaGradConfig{
                LearningRate: 0.01,
                Epsilon:      1e-10,
                WeightDecay:  0.0,
        }
}</span>

// NewAdaGradOptimizer creates a new GPU-resident AdaGrad optimizer
func NewAdaGradOptimizer(
        config AdaGradConfig,
        weightShapes [][]int,
        memoryManager *memory.MemoryManager,
        device unsafe.Pointer,
) (*AdaGradOptimizerState, error) <span class="cov0" title="0">{
        if memoryManager == nil </span><span class="cov0" title="0">{
                return nil, fmt.Errorf("memory manager cannot be nil")
        }</span>
        
        <span class="cov0" title="0">if device == nil </span><span class="cov0" title="0">{
                return nil, fmt.Errorf("device cannot be nil")
        }</span>
        
        <span class="cov0" title="0">if len(weightShapes) == 0 </span><span class="cov0" title="0">{
                return nil, fmt.Errorf("no weight shapes provided")
        }</span>
        
        <span class="cov0" title="0">numWeights := len(weightShapes)
        
        adagrad := &amp;AdaGradOptimizerState{
                config:                config,
                squaredGradAvgBuffers: make([]unsafe.Pointer, numWeights),
                WeightBuffers:         make([]unsafe.Pointer, numWeights),
                currentStep:           0,
                memoryManager:         memoryManager,
                device:                device,
                bufferSizes:           make([]int, numWeights),
        }
        
        // Calculate buffer sizes and allocate squared gradient average buffers
        for i, shape := range weightShapes </span><span class="cov0" title="0">{
                adagrad.bufferSizes[i] = calculateTensorSize(shape) * 4 // 4 bytes per float32
                
                // Allocate squared gradient average buffer
                squaredGradAvgBuffer := adagrad.memoryManager.AllocateBuffer(adagrad.bufferSizes[i])
                if squaredGradAvgBuffer == nil </span><span class="cov0" title="0">{
                        adagrad.cleanup()
                        return nil, fmt.Errorf("failed to allocate squared gradient average buffer for weight %d", i)
                }</span>
                <span class="cov0" title="0">adagrad.squaredGradAvgBuffers[i] = squaredGradAvgBuffer
                
                // Initialize to zero
                if err := cgo_bridge.ZeroMetalBuffer(adagrad.device, squaredGradAvgBuffer, adagrad.bufferSizes[i]); err != nil </span><span class="cov0" title="0">{
                        adagrad.cleanup()
                        return nil, fmt.Errorf("failed to zero squared gradient average buffer: %v", err)
                }</span>
        }
        
        <span class="cov0" title="0">return adagrad, nil</span>
}

// cleanup releases all allocated buffers
func (adagrad *AdaGradOptimizerState) cleanup() <span class="cov0" title="0">{
        for i := range adagrad.squaredGradAvgBuffers </span><span class="cov0" title="0">{
                if adagrad.squaredGradAvgBuffers[i] != nil </span><span class="cov0" title="0">{
                        adagrad.memoryManager.ReleaseBuffer(adagrad.squaredGradAvgBuffers[i])
                }</span>
        }
}

// SetWeightBuffers sets the current weight buffer pointers
func (adagrad *AdaGradOptimizerState) SetWeightBuffers(weightBuffers []unsafe.Pointer) error <span class="cov0" title="0">{
        if len(weightBuffers) != len(adagrad.WeightBuffers) </span><span class="cov0" title="0">{
                return fmt.Errorf("expected %d weight buffers, got %d", len(adagrad.WeightBuffers), len(weightBuffers))
        }</span>
        
        <span class="cov0" title="0">copy(adagrad.WeightBuffers, weightBuffers)
        return nil</span>
}

// Step performs a single AdaGrad optimization step
func (adagrad *AdaGradOptimizerState) Step(gradientBuffers []unsafe.Pointer) error <span class="cov0" title="0">{
        if len(gradientBuffers) != len(adagrad.WeightBuffers) </span><span class="cov0" title="0">{
                return fmt.Errorf("gradient buffers length (%d) doesn't match weight buffers length (%d)",
                        len(gradientBuffers), len(adagrad.WeightBuffers))
        }</span>
        
        <span class="cov0" title="0">adagrad.currentStep++
        
        // Execute AdaGrad step using CGO bridge
        var err error
        if adagrad.usePooling &amp;&amp; adagrad.commandPool != nil </span><span class="cov0" title="0">{
                err = cgo_bridge.ExecuteAdaGradStepMPSGraphPooled(
                        adagrad.device,
                        adagrad.WeightBuffers,
                        gradientBuffers,
                        adagrad.squaredGradAvgBuffers,
                        len(adagrad.WeightBuffers),
                        adagrad.bufferSizes,
                        adagrad.config.LearningRate,
                        adagrad.config.Epsilon,
                        adagrad.config.WeightDecay,
                        adagrad.commandPool,
                )
        }</span> else<span class="cov0" title="0"> {
                err = cgo_bridge.ExecuteAdaGradStepMPSGraph(
                        adagrad.device,
                        adagrad.WeightBuffers,
                        gradientBuffers,
                        adagrad.squaredGradAvgBuffers,
                        len(adagrad.WeightBuffers),
                        adagrad.bufferSizes,
                        adagrad.config.LearningRate,
                        adagrad.config.Epsilon,
                        adagrad.config.WeightDecay,
                )
        }</span>
        
        <span class="cov0" title="0">if err != nil </span><span class="cov0" title="0">{
                return fmt.Errorf("AdaGrad step failed: %v", err)
        }</span>
        
        // Log progress periodically
        <span class="cov0" title="0">if adagrad.currentStep%10 == 0 </span><span class="cov0" title="0">{
                fmt.Printf("AdaGrad step %d completed, lr=%.6f\n", adagrad.currentStep, adagrad.config.LearningRate)
        }</span>
        
        <span class="cov0" title="0">return nil</span>
}

// SetCommandPool sets the command buffer pool for Metal operations
func (adagrad *AdaGradOptimizerState) SetCommandPool(pool unsafe.Pointer) <span class="cov0" title="0">{
        adagrad.commandPool = pool
        adagrad.usePooling = (pool != nil)
}</span>

// GetStep returns the current optimization step count
func (adagrad *AdaGradOptimizerState) GetStep() uint64 <span class="cov0" title="0">{
        return adagrad.currentStep
}</span>

// GetStats returns optimizer statistics
func (adagrad *AdaGradOptimizerState) GetStats() map[string]interface{} <span class="cov0" title="0">{
        return map[string]interface{}{
                "step":          adagrad.currentStep,
                "learning_rate": adagrad.config.LearningRate,
                "epsilon":       adagrad.config.Epsilon,
                "weight_decay":  adagrad.config.WeightDecay,
        }
}</span>

// Cleanup releases all GPU buffers
func (adagrad *AdaGradOptimizerState) Cleanup() <span class="cov0" title="0">{
        adagrad.cleanup()
}</span>

// UpdateLearningRate updates the learning rate for the optimizer
func (adagrad *AdaGradOptimizerState) UpdateLearningRate(newLR float32) error <span class="cov0" title="0">{
        if newLR &lt;= 0 </span><span class="cov0" title="0">{
                return fmt.Errorf("learning rate must be positive, got %f", newLR)
        }</span>
        <span class="cov0" title="0">adagrad.config.LearningRate = newLR
        return nil</span>
}</pre>
		
		<pre class="file" id="file2" style="display: none">package optimizer

import (
        "fmt"
        "unsafe"

        "github.com/tsawler/go-metal/cgo_bridge"
        "github.com/tsawler/go-metal/memory"
)

// AdamOptimizerState represents GPU-resident Adam optimizer state
type AdamOptimizerState struct {
        // Hyperparameters
        LearningRate float32
        Beta1        float32 // Momentum decay (typically 0.9)
        Beta2        float32 // Variance decay (typically 0.999)
        Epsilon      float32 // Small constant to prevent division by zero (typically 1e-8)
        WeightDecay  float32 // L2 regularization coefficient

        // GPU-resident state buffers
        MomentumBuffers []unsafe.Pointer // First moment (momentum) for each weight tensor
        VarianceBuffers []unsafe.Pointer // Second moment (variance) for each weight tensor
        WeightBuffers   []unsafe.Pointer // Current weight tensors

        // Step tracking for bias correction
        StepCount uint64

        // Buffer management
        memoryManager *memory.MemoryManager
        device        unsafe.Pointer

        // Buffer sizes for proper cleanup
        bufferSizes []int
        
        // RESOURCE LEAK FIX: Command buffer pooling
        commandPool unsafe.Pointer  // Optional command buffer pool for Metal operations
        usePooling  bool           // Whether to use command buffer pooling
}

// AdamConfig holds configuration for Adam optimizer
type AdamConfig struct {
        LearningRate float32
        Beta1        float32
        Beta2        float32
        Epsilon      float32
        WeightDecay  float32
}

// DefaultAdamConfig returns default Adam optimizer configuration
func DefaultAdamConfig() AdamConfig <span class="cov8" title="1">{
        return AdamConfig{
                LearningRate: 0.001,
                Beta1:        0.9,
                Beta2:        0.999,
                Epsilon:      1e-8,
                WeightDecay:  0.0,
        }
}</span>

// NewAdamOptimizer creates a new GPU-resident Adam optimizer
func NewAdamOptimizer(
        config AdamConfig,
        weightShapes [][]int,
        memoryManager *memory.MemoryManager,
        device unsafe.Pointer,
) (*AdamOptimizerState, error) <span class="cov0" title="0">{
        if memoryManager == nil </span><span class="cov0" title="0">{
                return nil, fmt.Errorf("memory manager cannot be nil")
        }</span>

        <span class="cov0" title="0">if device == nil </span><span class="cov0" title="0">{
                return nil, fmt.Errorf("device cannot be nil")
        }</span>

        <span class="cov0" title="0">if len(weightShapes) == 0 </span><span class="cov0" title="0">{
                return nil, fmt.Errorf("no weight shapes provided")
        }</span>

        <span class="cov0" title="0">numWeights := len(weightShapes)

        adam := &amp;AdamOptimizerState{
                LearningRate:    config.LearningRate,
                Beta1:           config.Beta1,
                Beta2:           config.Beta2,
                Epsilon:         config.Epsilon,
                WeightDecay:     config.WeightDecay,
                MomentumBuffers: make([]unsafe.Pointer, numWeights),
                VarianceBuffers: make([]unsafe.Pointer, numWeights),
                WeightBuffers:   make([]unsafe.Pointer, numWeights),
                StepCount:       0,
                memoryManager:   memoryManager,
                device:          device,
                bufferSizes:     make([]int, numWeights),
        }

        // Allocate GPU buffers for momentum and variance
        for i, shape := range weightShapes </span><span class="cov0" title="0">{
                // Calculate buffer size (assume float32)
                size := calculateTensorSize(shape) * 4 // 4 bytes per float32
                adam.bufferSizes[i] = size

                // Allocate momentum buffer
                momentumBuffer := adam.memoryManager.AllocateBuffer(size)
                if momentumBuffer == nil </span><span class="cov0" title="0">{
                        adam.cleanup(i) // Cleanup previously allocated buffers
                        return nil, fmt.Errorf("failed to allocate momentum buffer for weight %d", i)
                }</span>
                <span class="cov0" title="0">adam.MomentumBuffers[i] = momentumBuffer

                // Allocate variance buffer
                varianceBuffer := adam.memoryManager.AllocateBuffer(size)
                if varianceBuffer == nil </span><span class="cov0" title="0">{
                        adam.memoryManager.ReleaseBuffer(momentumBuffer)
                        adam.cleanup(i) // Cleanup previously allocated buffers
                        return nil, fmt.Errorf("failed to allocate variance buffer for weight %d", i)
                }</span>
                <span class="cov0" title="0">adam.VarianceBuffers[i] = varianceBuffer

                // Initialize buffers to zero (momentum and variance start at 0)
                err := cgo_bridge.ZeroMetalBuffer(adam.device, momentumBuffer, size)
                if err != nil </span><span class="cov0" title="0">{
                        adam.memoryManager.ReleaseBuffer(momentumBuffer)
                        adam.memoryManager.ReleaseBuffer(varianceBuffer)
                        adam.cleanup(i)
                        return nil, fmt.Errorf("failed to zero momentum buffer for weight %d: %v", i, err)
                }</span>

                <span class="cov0" title="0">err = cgo_bridge.ZeroMetalBuffer(adam.device, varianceBuffer, size)
                if err != nil </span><span class="cov0" title="0">{
                        adam.memoryManager.ReleaseBuffer(momentumBuffer)
                        adam.memoryManager.ReleaseBuffer(varianceBuffer)
                        adam.cleanup(i)
                        return nil, fmt.Errorf("failed to zero variance buffer for weight %d: %v", i, err)
                }</span>
        }

        <span class="cov0" title="0">return adam, nil</span>
}

// calculateTensorSize calculates the number of elements in a tensor
func calculateTensorSize(shape []int) int <span class="cov8" title="1">{
        size := 1
        for _, dim := range shape </span><span class="cov8" title="1">{
                size *= dim
        }</span>
        <span class="cov8" title="1">return size</span>
}

// cleanup releases previously allocated buffers in case of partial initialization failure
func (adam *AdamOptimizerState) cleanup(upToIndex int) <span class="cov0" title="0">{
        for i := 0; i &lt; upToIndex; i++ </span><span class="cov0" title="0">{
                if adam.MomentumBuffers[i] != nil </span><span class="cov0" title="0">{
                        adam.memoryManager.ReleaseBuffer(adam.MomentumBuffers[i])
                        adam.MomentumBuffers[i] = nil
                }</span>
                <span class="cov0" title="0">if adam.VarianceBuffers[i] != nil </span><span class="cov0" title="0">{
                        adam.memoryManager.ReleaseBuffer(adam.VarianceBuffers[i])
                        adam.VarianceBuffers[i] = nil
                }</span>
        }
}

// SetWeightBuffers sets the current weight buffer pointers
// This should be called before each optimization step
func (adam *AdamOptimizerState) SetWeightBuffers(weightBuffers []unsafe.Pointer) error <span class="cov0" title="0">{
        if len(weightBuffers) != len(adam.WeightBuffers) </span><span class="cov0" title="0">{
                return fmt.Errorf("expected %d weight buffers, got %d", len(adam.WeightBuffers), len(weightBuffers))
        }</span>

        <span class="cov0" title="0">copy(adam.WeightBuffers, weightBuffers)
        return nil</span>
}

// Step performs a single Adam optimization step
// This will be implemented as a Metal compute kernel for maximum performance
func (adam *AdamOptimizerState) Step(gradientBuffers []unsafe.Pointer) error <span class="cov0" title="0">{
        if len(gradientBuffers) != len(adam.WeightBuffers) </span><span class="cov0" title="0">{
                return fmt.Errorf("gradient buffers length (%d) doesn't match weight buffers length (%d)",
                        len(gradientBuffers), len(adam.WeightBuffers))
        }</span>

        <span class="cov0" title="0">adam.StepCount++

        // DEBUG: Add logging to verify Adam is being called
        // if adam.StepCount%10 == 1 {
        //        fmt.Printf("🔧 Adam step %d: lr=%.6f, %d weights, %d gradients\n", 
        //                adam.StepCount, adam.LearningRate, len(adam.WeightBuffers), len(gradientBuffers))
        // }

        // DEBUG: Temporarily disable pooled Adam to test if non-pooled works
        // Learning broke again - need to isolate issue
        var err error
        // if adam.usePooling &amp;&amp; adam.commandPool != nil {
        //         // Use pooled command buffers with proper bias correction
        //         err = cgo_bridge.ExecuteAdamStepMPSGraphPooled(
        //                 adam.device,
        //                 adam.WeightBuffers,
        //                 gradientBuffers,
        //                 adam.MomentumBuffers,
        //                 adam.VarianceBuffers,
        //                 adam.bufferSizes,
        //                 adam.LearningRate,
        //                 adam.Beta1,
        //                 adam.Beta2,
        //                 adam.Epsilon,
        //                 adam.WeightDecay,
        //                 int(adam.StepCount),
        //                 adam.commandPool,
        //         )
        // } else {
                // Force non-pooled version to test learning
                err = cgo_bridge.ExecuteAdamStepMPSGraph(
                        adam.device,
                        adam.WeightBuffers,
                        gradientBuffers,
                        adam.MomentumBuffers,
                        adam.VarianceBuffers,
                        adam.bufferSizes,
                        adam.LearningRate,
                        adam.Beta1,
                        adam.Beta2,
                        adam.Epsilon,
                        adam.WeightDecay,
                        int(adam.StepCount),
                )
        // }

        if err != nil </span><span class="cov0" title="0">{
                return fmt.Errorf("Adam step execution failed: %v", err)
        }</span>

        <span class="cov0" title="0">return nil</span>
}

// pow computes x^y for float32
func pow(x, y float32) float32 <span class="cov0" title="0">{
        if y == 0 </span><span class="cov0" title="0">{
                return 1.0
        }</span>
        <span class="cov0" title="0">if y == 1 </span><span class="cov0" title="0">{
                return x
        }</span>
        
        // Simple implementation for small integer powers
        <span class="cov0" title="0">result := float32(1.0)
        for i := 0; i &lt; int(y); i++ </span><span class="cov0" title="0">{
                result *= x
        }</span>
        <span class="cov0" title="0">return result</span>
}

// UpdateLearningRate updates the learning rate (useful for learning rate scheduling)
func (adam *AdamOptimizerState) UpdateLearningRate(newLR float32) <span class="cov8" title="1">{
        adam.LearningRate = newLR
}</span>

// SetCommandPool enables command buffer pooling for Metal operations
// RESOURCE LEAK FIX: Allows Adam optimizer to use pooled command buffers
func (adam *AdamOptimizerState) SetCommandPool(commandPool unsafe.Pointer) <span class="cov0" title="0">{
        adam.commandPool = commandPool
        adam.usePooling = (commandPool != nil)
}</span>

// GetStep returns the current step count
func (adam *AdamOptimizerState) GetStep() uint64 <span class="cov8" title="1">{
        return adam.StepCount
}</span>

// GetStats returns optimizer statistics
func (adam *AdamOptimizerState) GetStats() AdamStats <span class="cov8" title="1">{
        return AdamStats{
                StepCount:       adam.StepCount,
                LearningRate:    adam.LearningRate,
                Beta1:           adam.Beta1,
                Beta2:           adam.Beta2,
                Epsilon:         adam.Epsilon,
                WeightDecay:     adam.WeightDecay,
                NumParameters:   len(adam.WeightBuffers),
                TotalBufferSize: adam.getTotalBufferSize(),
        }
}</span>

// AdamStats provides statistics about the Adam optimizer
type AdamStats struct {
        StepCount       uint64
        LearningRate    float32
        Beta1           float32
        Beta2           float32
        Epsilon         float32
        WeightDecay     float32
        NumParameters   int
        TotalBufferSize int
}

// getTotalBufferSize calculates total memory used by optimizer state
func (adam *AdamOptimizerState) getTotalBufferSize() int <span class="cov8" title="1">{
        total := 0
        for _, size := range adam.bufferSizes </span><span class="cov8" title="1">{
                total += size * 2 // momentum + variance buffers
        }</span>
        <span class="cov8" title="1">return total</span>
}

// Cleanup releases all GPU buffers
func (adam *AdamOptimizerState) Cleanup() <span class="cov0" title="0">{
        for i := range adam.MomentumBuffers </span><span class="cov0" title="0">{
                if adam.MomentumBuffers[i] != nil </span><span class="cov0" title="0">{
                        adam.memoryManager.ReleaseBuffer(adam.MomentumBuffers[i])
                        adam.MomentumBuffers[i] = nil
                }</span>
                <span class="cov0" title="0">if adam.VarianceBuffers[i] != nil </span><span class="cov0" title="0">{
                        adam.memoryManager.ReleaseBuffer(adam.VarianceBuffers[i])
                        adam.VarianceBuffers[i] = nil
                }</span>
        }
        
        // Clear slices
        <span class="cov0" title="0">adam.MomentumBuffers = nil
        adam.VarianceBuffers = nil
        adam.WeightBuffers = nil
        adam.bufferSizes = nil</span>
}</pre>
		
		<pre class="file" id="file3" style="display: none">package optimizer

import (
        "fmt"
        "unsafe"

        "github.com/tsawler/go-metal/cgo_bridge"
        "github.com/tsawler/go-metal/memory"
)

// LBFGSOptimizerState represents GPU-resident L-BFGS optimizer state
type LBFGSOptimizerState struct {
        // Configuration
        config LBFGSConfig
        
        // GPU-resident state buffers
        sVectors      [][]unsafe.Pointer // Parameter differences s_k = x_{k+1} - x_k
        yVectors      [][]unsafe.Pointer // Gradient differences y_k = g_{k+1} - g_k
        rhoBuffers    []unsafe.Pointer   // Scalar values ρ_k = 1/(y_k^T s_k)
        alphaBuffer   unsafe.Pointer     // Alpha values for two-loop recursion
        oldGradients  []unsafe.Pointer   // Previous gradients for computing y_k
        searchDir     []unsafe.Pointer   // Search direction p_k
        WeightBuffers []unsafe.Pointer   // Current weight tensors
        
        // History tracking
        currentStep  uint64
        historyCount int  // Current number of stored history pairs
        historyIndex int  // Circular buffer index
        
        // Buffer management
        memoryManager *memory.MemoryManager
        device        unsafe.Pointer
        bufferSizes   []int
        
        // Command buffer pooling
        commandPool unsafe.Pointer
        usePooling  bool
        
        // Line search state
        prevLoss     float32
        prevGradNorm float32
}

// LBFGSConfig holds configuration for L-BFGS optimizer
type LBFGSConfig struct {
        HistorySize   int     // m parameter (number of corrections to store)
        LineSearchTol float32 // Tolerance for line search
        MaxLineSearch int     // Maximum line search iterations
        C1            float32 // Armijo condition parameter
        C2            float32 // Wolfe condition parameter
        InitialStep   float32 // Initial step size for line search
}

// DefaultLBFGSConfig returns default L-BFGS optimizer configuration
func DefaultLBFGSConfig() LBFGSConfig <span class="cov0" title="0">{
        return LBFGSConfig{
                HistorySize:   10,
                LineSearchTol: 1e-4,
                MaxLineSearch: 20,
                C1:            1e-4,
                C2:            0.9,
                InitialStep:   1.0,
        }
}</span>

// NewLBFGSOptimizer creates a new GPU-resident L-BFGS optimizer
func NewLBFGSOptimizer(
        config LBFGSConfig,
        weightShapes [][]int,
        memoryManager *memory.MemoryManager,
        device unsafe.Pointer,
) (*LBFGSOptimizerState, error) <span class="cov0" title="0">{
        if memoryManager == nil </span><span class="cov0" title="0">{
                return nil, fmt.Errorf("memory manager cannot be nil")
        }</span>
        
        <span class="cov0" title="0">if device == nil </span><span class="cov0" title="0">{
                return nil, fmt.Errorf("device cannot be nil")
        }</span>
        
        <span class="cov0" title="0">if len(weightShapes) == 0 </span><span class="cov0" title="0">{
                return nil, fmt.Errorf("no weight shapes provided")
        }</span>
        
        <span class="cov0" title="0">if config.HistorySize &lt;= 0 </span><span class="cov0" title="0">{
                return nil, fmt.Errorf("history size must be positive, got %d", config.HistorySize)
        }</span>
        
        <span class="cov0" title="0">numWeights := len(weightShapes)
        
        lbfgs := &amp;LBFGSOptimizerState{
                config:        config,
                sVectors:      make([][]unsafe.Pointer, config.HistorySize),
                yVectors:      make([][]unsafe.Pointer, config.HistorySize),
                rhoBuffers:    make([]unsafe.Pointer, config.HistorySize),
                oldGradients:  make([]unsafe.Pointer, numWeights),
                searchDir:     make([]unsafe.Pointer, numWeights),
                WeightBuffers: make([]unsafe.Pointer, numWeights),
                currentStep:   0,
                historyCount:  0,
                historyIndex:  0,
                memoryManager: memoryManager,
                device:        device,
                bufferSizes:   make([]int, numWeights),
        }
        
        // Calculate buffer sizes
        for i, shape := range weightShapes </span><span class="cov0" title="0">{
                lbfgs.bufferSizes[i] = calculateTensorSize(shape) * 4 // 4 bytes per float32
        }</span>
        
        // Allocate history vectors (s and y) for circular buffer
        <span class="cov0" title="0">for h := 0; h &lt; config.HistorySize; h++ </span><span class="cov0" title="0">{
                lbfgs.sVectors[h] = make([]unsafe.Pointer, numWeights)
                lbfgs.yVectors[h] = make([]unsafe.Pointer, numWeights)
                
                // Allocate buffers for each weight tensor
                for i, size := range lbfgs.bufferSizes </span><span class="cov0" title="0">{
                        // Allocate s vector
                        sBuffer := lbfgs.memoryManager.AllocateBuffer(size)
                        if sBuffer == nil </span><span class="cov0" title="0">{
                                lbfgs.cleanup()
                                return nil, fmt.Errorf("failed to allocate s vector buffer for history %d, weight %d", h, i)
                        }</span>
                        <span class="cov0" title="0">lbfgs.sVectors[h][i] = sBuffer
                        
                        // Allocate y vector
                        yBuffer := lbfgs.memoryManager.AllocateBuffer(size)
                        if yBuffer == nil </span><span class="cov0" title="0">{
                                lbfgs.cleanup()
                                return nil, fmt.Errorf("failed to allocate y vector buffer for history %d, weight %d", h, i)
                        }</span>
                        <span class="cov0" title="0">lbfgs.yVectors[h][i] = yBuffer
                        
                        // Initialize to zero
                        if err := cgo_bridge.ZeroMetalBuffer(lbfgs.device, sBuffer, size); err != nil </span><span class="cov0" title="0">{
                                lbfgs.cleanup()
                                return nil, fmt.Errorf("failed to zero s buffer: %v", err)
                        }</span>
                        <span class="cov0" title="0">if err := cgo_bridge.ZeroMetalBuffer(lbfgs.device, yBuffer, size); err != nil </span><span class="cov0" title="0">{
                                lbfgs.cleanup()
                                return nil, fmt.Errorf("failed to zero y buffer: %v", err)
                        }</span>
                }
                
                // Allocate rho scalar buffer (single float32)
                <span class="cov0" title="0">rhoBuffer := lbfgs.memoryManager.AllocateBuffer(4)
                if rhoBuffer == nil </span><span class="cov0" title="0">{
                        lbfgs.cleanup()
                        return nil, fmt.Errorf("failed to allocate rho buffer for history %d", h)
                }</span>
                <span class="cov0" title="0">lbfgs.rhoBuffers[h] = rhoBuffer</span>
        }
        
        // Allocate alpha buffer for two-loop recursion
        <span class="cov0" title="0">alphaSize := config.HistorySize * 4 // m float32 values
        lbfgs.alphaBuffer = lbfgs.memoryManager.AllocateBuffer(alphaSize)
        if lbfgs.alphaBuffer == nil </span><span class="cov0" title="0">{
                lbfgs.cleanup()
                return nil, fmt.Errorf("failed to allocate alpha buffer")
        }</span>
        
        // Allocate old gradients and search direction buffers
        <span class="cov0" title="0">for i, size := range lbfgs.bufferSizes </span><span class="cov0" title="0">{
                // Old gradients
                oldGradBuffer := lbfgs.memoryManager.AllocateBuffer(size)
                if oldGradBuffer == nil </span><span class="cov0" title="0">{
                        lbfgs.cleanup()
                        return nil, fmt.Errorf("failed to allocate old gradient buffer for weight %d", i)
                }</span>
                <span class="cov0" title="0">lbfgs.oldGradients[i] = oldGradBuffer
                
                // Search direction
                searchDirBuffer := lbfgs.memoryManager.AllocateBuffer(size)
                if searchDirBuffer == nil </span><span class="cov0" title="0">{
                        lbfgs.cleanup()
                        return nil, fmt.Errorf("failed to allocate search direction buffer for weight %d", i)
                }</span>
                <span class="cov0" title="0">lbfgs.searchDir[i] = searchDirBuffer
                
                // Initialize to zero
                if err := cgo_bridge.ZeroMetalBuffer(lbfgs.device, oldGradBuffer, size); err != nil </span><span class="cov0" title="0">{
                        lbfgs.cleanup()
                        return nil, fmt.Errorf("failed to zero old gradient buffer: %v", err)
                }</span>
                <span class="cov0" title="0">if err := cgo_bridge.ZeroMetalBuffer(lbfgs.device, searchDirBuffer, size); err != nil </span><span class="cov0" title="0">{
                        lbfgs.cleanup()
                        return nil, fmt.Errorf("failed to zero search direction buffer: %v", err)
                }</span>
        }
        
        <span class="cov0" title="0">return lbfgs, nil</span>
}

// cleanup releases all allocated buffers
func (lbfgs *LBFGSOptimizerState) cleanup() <span class="cov0" title="0">{
        // Clean up history vectors
        for h := 0; h &lt; lbfgs.config.HistorySize; h++ </span><span class="cov0" title="0">{
                if lbfgs.sVectors[h] != nil </span><span class="cov0" title="0">{
                        for i := 0; i &lt; len(lbfgs.WeightBuffers); i++ </span><span class="cov0" title="0">{
                                if lbfgs.sVectors[h][i] != nil </span><span class="cov0" title="0">{
                                        lbfgs.memoryManager.ReleaseBuffer(lbfgs.sVectors[h][i])
                                }</span>
                        }
                }
                <span class="cov0" title="0">if lbfgs.yVectors[h] != nil </span><span class="cov0" title="0">{
                        for i := 0; i &lt; len(lbfgs.WeightBuffers); i++ </span><span class="cov0" title="0">{
                                if lbfgs.yVectors[h][i] != nil </span><span class="cov0" title="0">{
                                        lbfgs.memoryManager.ReleaseBuffer(lbfgs.yVectors[h][i])
                                }</span>
                        }
                }
                <span class="cov0" title="0">if lbfgs.rhoBuffers[h] != nil </span><span class="cov0" title="0">{
                        lbfgs.memoryManager.ReleaseBuffer(lbfgs.rhoBuffers[h])
                }</span>
        }
        
        // Clean up alpha buffer
        <span class="cov0" title="0">if lbfgs.alphaBuffer != nil </span><span class="cov0" title="0">{
                lbfgs.memoryManager.ReleaseBuffer(lbfgs.alphaBuffer)
        }</span>
        
        // Clean up old gradients and search direction
        <span class="cov0" title="0">for i := 0; i &lt; len(lbfgs.WeightBuffers); i++ </span><span class="cov0" title="0">{
                if lbfgs.oldGradients[i] != nil </span><span class="cov0" title="0">{
                        lbfgs.memoryManager.ReleaseBuffer(lbfgs.oldGradients[i])
                }</span>
                <span class="cov0" title="0">if lbfgs.searchDir[i] != nil </span><span class="cov0" title="0">{
                        lbfgs.memoryManager.ReleaseBuffer(lbfgs.searchDir[i])
                }</span>
        }
}

// SetWeightBuffers sets the current weight buffer pointers
func (lbfgs *LBFGSOptimizerState) SetWeightBuffers(weightBuffers []unsafe.Pointer) error <span class="cov0" title="0">{
        if len(weightBuffers) != len(lbfgs.WeightBuffers) </span><span class="cov0" title="0">{
                return fmt.Errorf("expected %d weight buffers, got %d", len(lbfgs.WeightBuffers), len(weightBuffers))
        }</span>
        
        <span class="cov0" title="0">copy(lbfgs.WeightBuffers, weightBuffers)
        return nil</span>
}

// Step performs a single L-BFGS optimization step
func (lbfgs *LBFGSOptimizerState) Step(gradientBuffers []unsafe.Pointer, currentLoss float32) error <span class="cov0" title="0">{
        if len(gradientBuffers) != len(lbfgs.WeightBuffers) </span><span class="cov0" title="0">{
                return fmt.Errorf("gradient buffers length (%d) doesn't match weight buffers length (%d)",
                        len(gradientBuffers), len(lbfgs.WeightBuffers))
        }</span>
        
        <span class="cov0" title="0">lbfgs.currentStep++
        
        // Execute L-BFGS step using CGO bridge
        stepSize, err := cgo_bridge.ExecuteLBFGSStepMPSGraph(
                lbfgs.device,
                lbfgs.WeightBuffers,
                gradientBuffers,
                lbfgs.oldGradients,
                lbfgs.searchDir,
                lbfgs.sVectors,
                lbfgs.yVectors,
                lbfgs.rhoBuffers,
                lbfgs.alphaBuffer,
                len(lbfgs.WeightBuffers),
                lbfgs.bufferSizes,
                lbfgs.config.HistorySize,
                lbfgs.historyCount,
                lbfgs.historyIndex,
                lbfgs.config.InitialStep,
                lbfgs.config.C1,
                lbfgs.config.C2,
                lbfgs.config.MaxLineSearch,
                currentLoss,
                lbfgs.prevLoss,
                lbfgs.commandPool,
                lbfgs.usePooling,
        )
        
        if err != nil </span><span class="cov0" title="0">{
                return fmt.Errorf("L-BFGS step failed: %v", err)
        }</span>
        
        // Note: y_k and rho_k computation is now handled entirely in the C code
        // to avoid race conditions and ensure proper buffer management
        
        // Update history tracking
        <span class="cov0" title="0">lbfgs.historyIndex = (lbfgs.historyIndex + 1) % lbfgs.config.HistorySize
        if lbfgs.historyCount &lt; lbfgs.config.HistorySize </span><span class="cov0" title="0">{
                lbfgs.historyCount++
        }</span>
        
        <span class="cov0" title="0">lbfgs.prevLoss = currentLoss
        
        // Log progress periodically
        if lbfgs.currentStep%10 == 0 </span><span class="cov0" title="0">{
                fmt.Printf("L-BFGS step %d: loss=%.6f, step_size=%.6f, history=%d/%d\n",
                        lbfgs.currentStep, currentLoss, stepSize, lbfgs.historyCount, lbfgs.config.HistorySize)
        }</span>
        
        <span class="cov0" title="0">return nil</span>
}

// SetCommandPool sets the command buffer pool for Metal operations
func (lbfgs *LBFGSOptimizerState) SetCommandPool(pool unsafe.Pointer) <span class="cov0" title="0">{
        lbfgs.commandPool = pool
        lbfgs.usePooling = (pool != nil)
}</span>

// GetStep returns the current optimization step count
func (lbfgs *LBFGSOptimizerState) GetStep() uint64 <span class="cov0" title="0">{
        return lbfgs.currentStep
}</span>

// GetStats returns optimizer statistics
func (lbfgs *LBFGSOptimizerState) GetStats() map[string]interface{} <span class="cov0" title="0">{
        return map[string]interface{}{
                "step":         lbfgs.currentStep,
                "history_size": lbfgs.config.HistorySize,
                "history_used": lbfgs.historyCount,
                "prev_loss":    lbfgs.prevLoss,
        }
}</span>

// Cleanup releases all GPU buffers
func (lbfgs *LBFGSOptimizerState) Cleanup() <span class="cov0" title="0">{
        lbfgs.cleanup()
}</span>

// UpdateLearningRate is not used in L-BFGS (uses line search instead)
func (lbfgs *LBFGSOptimizerState) UpdateLearningRate(newLR float32) error <span class="cov0" title="0">{
        return fmt.Errorf("L-BFGS does not use a fixed learning rate; it uses line search")
}</pre>
		
		<pre class="file" id="file4" style="display: none">package optimizer

import (
        "fmt"
        "unsafe"

        "github.com/tsawler/go-metal/cgo_bridge"
        "github.com/tsawler/go-metal/memory"
)

// NadamOptimizerState represents GPU-resident Nadam optimizer state
// Nadam combines Adam's adaptive learning rates with Nesterov momentum
type NadamOptimizerState struct {
        config NadamConfig

        // GPU-resident state buffers
        momentumBuffers []unsafe.Pointer // First moment (momentum) for each weight tensor
        varianceBuffers []unsafe.Pointer // Second moment (variance) for each weight tensor
        WeightBuffers   []unsafe.Pointer // Current weight tensors

        // Step tracking for bias correction
        currentStep uint64

        // Buffer management
        memoryManager *memory.MemoryManager
        device        unsafe.Pointer

        // Buffer sizes for proper cleanup
        bufferSizes []int

        // Command buffer pooling
        commandPool unsafe.Pointer // Optional command buffer pool for Metal operations
        usePooling  bool           // Whether to use command buffer pooling
}

// NadamConfig holds configuration for Nadam optimizer
type NadamConfig struct {
        LearningRate float32 // Base learning rate (typically 0.002)
        Beta1        float32 // Exponential decay rate for first moment estimates (typically 0.9)
        Beta2        float32 // Exponential decay rate for second moment estimates (typically 0.999)
        Epsilon      float32 // Small constant for numerical stability (typically 1e-8)
        WeightDecay  float32 // L2 regularization coefficient (typically 0.0)
}

// DefaultNadamConfig returns default Nadam optimizer configuration
func DefaultNadamConfig() NadamConfig <span class="cov8" title="1">{
        return NadamConfig{
                LearningRate: 0.002, // Nadam typically uses slightly higher LR than Adam
                Beta1:        0.9,
                Beta2:        0.999,
                Epsilon:      1e-8,
                WeightDecay:  0.0,
        }
}</span>

// NewNadamOptimizer creates a new GPU-resident Nadam optimizer
func NewNadamOptimizer(
        config NadamConfig,
        weightShapes [][]int,
        memoryManager *memory.MemoryManager,
        device unsafe.Pointer,
) (*NadamOptimizerState, error) <span class="cov0" title="0">{
        if memoryManager == nil </span><span class="cov0" title="0">{
                return nil, fmt.Errorf("memory manager cannot be nil")
        }</span>

        <span class="cov0" title="0">if device == nil </span><span class="cov0" title="0">{
                return nil, fmt.Errorf("device cannot be nil")
        }</span>

        <span class="cov0" title="0">if len(weightShapes) == 0 </span><span class="cov0" title="0">{
                return nil, fmt.Errorf("no weight shapes provided")
        }</span>

        // Validate configuration
        <span class="cov0" title="0">if config.LearningRate &lt;= 0 </span><span class="cov0" title="0">{
                return nil, fmt.Errorf("learning rate must be positive, got %f", config.LearningRate)
        }</span>
        <span class="cov0" title="0">if config.Beta1 &lt; 0 || config.Beta1 &gt;= 1 </span><span class="cov0" title="0">{
                return nil, fmt.Errorf("beta1 must be in [0, 1), got %f", config.Beta1)
        }</span>
        <span class="cov0" title="0">if config.Beta2 &lt; 0 || config.Beta2 &gt;= 1 </span><span class="cov0" title="0">{
                return nil, fmt.Errorf("beta2 must be in [0, 1), got %f", config.Beta2)
        }</span>
        <span class="cov0" title="0">if config.Epsilon &lt;= 0 </span><span class="cov0" title="0">{
                return nil, fmt.Errorf("epsilon must be positive, got %e", config.Epsilon)
        }</span>
        <span class="cov0" title="0">if config.WeightDecay &lt; 0 </span><span class="cov0" title="0">{
                return nil, fmt.Errorf("weight decay must be non-negative, got %f", config.WeightDecay)
        }</span>

        <span class="cov0" title="0">numWeights := len(weightShapes)

        nadam := &amp;NadamOptimizerState{
                config:          config,
                momentumBuffers: make([]unsafe.Pointer, numWeights),
                varianceBuffers: make([]unsafe.Pointer, numWeights),
                WeightBuffers:   make([]unsafe.Pointer, numWeights),
                currentStep:     0,
                memoryManager:   memoryManager,
                device:          device,
                bufferSizes:     make([]int, numWeights),
        }

        // Allocate GPU buffers for momentum and variance
        for i, shape := range weightShapes </span><span class="cov0" title="0">{
                // Calculate buffer size (assume float32)
                size := calculateTensorSize(shape) * 4 // 4 bytes per float32
                nadam.bufferSizes[i] = size

                // Allocate momentum buffer
                momentumBuffer := nadam.memoryManager.AllocateBuffer(size)
                if momentumBuffer == nil </span><span class="cov0" title="0">{
                        nadam.cleanup(i) // Cleanup previously allocated buffers
                        return nil, fmt.Errorf("failed to allocate momentum buffer for weight %d", i)
                }</span>
                <span class="cov0" title="0">nadam.momentumBuffers[i] = momentumBuffer

                // Allocate variance buffer
                varianceBuffer := nadam.memoryManager.AllocateBuffer(size)
                if varianceBuffer == nil </span><span class="cov0" title="0">{
                        nadam.memoryManager.ReleaseBuffer(momentumBuffer)
                        nadam.cleanup(i) // Cleanup previously allocated buffers
                        return nil, fmt.Errorf("failed to allocate variance buffer for weight %d", i)
                }</span>
                <span class="cov0" title="0">nadam.varianceBuffers[i] = varianceBuffer

                // Initialize buffers to zero (momentum and variance start at 0)
                err := cgo_bridge.ZeroMetalBuffer(nadam.device, momentumBuffer, size)
                if err != nil </span><span class="cov0" title="0">{
                        nadam.memoryManager.ReleaseBuffer(momentumBuffer)
                        nadam.memoryManager.ReleaseBuffer(varianceBuffer)
                        nadam.cleanup(i)
                        return nil, fmt.Errorf("failed to zero momentum buffer for weight %d: %v", i, err)
                }</span>

                <span class="cov0" title="0">err = cgo_bridge.ZeroMetalBuffer(nadam.device, varianceBuffer, size)
                if err != nil </span><span class="cov0" title="0">{
                        nadam.memoryManager.ReleaseBuffer(momentumBuffer)
                        nadam.memoryManager.ReleaseBuffer(varianceBuffer)
                        nadam.cleanup(i)
                        return nil, fmt.Errorf("failed to zero variance buffer for weight %d: %v", i, err)
                }</span>
        }

        <span class="cov0" title="0">return nadam, nil</span>
}

// cleanup releases previously allocated buffers in case of partial initialization failure
func (nadam *NadamOptimizerState) cleanup(upToIndex int) <span class="cov0" title="0">{
        for i := 0; i &lt; upToIndex; i++ </span><span class="cov0" title="0">{
                if nadam.momentumBuffers[i] != nil </span><span class="cov0" title="0">{
                        nadam.memoryManager.ReleaseBuffer(nadam.momentumBuffers[i])
                        nadam.momentumBuffers[i] = nil
                }</span>
                <span class="cov0" title="0">if nadam.varianceBuffers[i] != nil </span><span class="cov0" title="0">{
                        nadam.memoryManager.ReleaseBuffer(nadam.varianceBuffers[i])
                        nadam.varianceBuffers[i] = nil
                }</span>
        }
}

// SetWeightBuffers sets the current weight buffer pointers
// This should be called before each optimization step
func (nadam *NadamOptimizerState) SetWeightBuffers(weightBuffers []unsafe.Pointer) error <span class="cov0" title="0">{
        if len(weightBuffers) != len(nadam.WeightBuffers) </span><span class="cov0" title="0">{
                return fmt.Errorf("expected %d weight buffers, got %d", len(nadam.WeightBuffers), len(weightBuffers))
        }</span>

        <span class="cov0" title="0">copy(nadam.WeightBuffers, weightBuffers)
        return nil</span>
}

// Step performs a single Nadam optimization step
// Nadam combines Adam's adaptive learning rates with Nesterov momentum
func (nadam *NadamOptimizerState) Step(gradientBuffers []unsafe.Pointer) error <span class="cov0" title="0">{
        if len(gradientBuffers) != len(nadam.WeightBuffers) </span><span class="cov0" title="0">{
                return fmt.Errorf("gradient buffers length (%d) doesn't match weight buffers length (%d)",
                        len(gradientBuffers), len(nadam.WeightBuffers))
        }</span>

        <span class="cov0" title="0">nadam.currentStep++

        // Execute Nadam step using MPSGraph
        err := cgo_bridge.ExecuteNadamStepMPSGraph(
                nadam.device,
                nadam.WeightBuffers,
                gradientBuffers,
                nadam.momentumBuffers,
                nadam.varianceBuffers,
                nadam.bufferSizes,
                nadam.config.LearningRate,
                nadam.config.Beta1,
                nadam.config.Beta2,
                nadam.config.Epsilon,
                nadam.config.WeightDecay,
                int(nadam.currentStep),
        )

        if err != nil </span><span class="cov0" title="0">{
                return fmt.Errorf("Nadam step execution failed: %v", err)
        }</span>

        <span class="cov0" title="0">return nil</span>
}

// UpdateLearningRate updates the learning rate (useful for learning rate scheduling)
func (nadam *NadamOptimizerState) UpdateLearningRate(newLR float32) error <span class="cov0" title="0">{
        if newLR &lt;= 0 </span><span class="cov0" title="0">{
                return fmt.Errorf("learning rate must be positive, got %f", newLR)
        }</span>
        <span class="cov0" title="0">nadam.config.LearningRate = newLR
        return nil</span>
}

// SetCommandPool enables command buffer pooling for Metal operations
func (nadam *NadamOptimizerState) SetCommandPool(commandPool unsafe.Pointer) <span class="cov0" title="0">{
        nadam.commandPool = commandPool
        nadam.usePooling = (commandPool != nil)
}</span>

// GetStep returns the current step count
func (nadam *NadamOptimizerState) GetStep() uint64 <span class="cov0" title="0">{
        return nadam.currentStep
}</span>

// GetStats returns optimizer statistics as a map for generic access
func (nadam *NadamOptimizerState) GetStats() map[string]interface{} <span class="cov0" title="0">{
        return map[string]interface{}{
                "step":          nadam.currentStep,
                "learning_rate": nadam.config.LearningRate,
                "beta1":         nadam.config.Beta1,
                "beta2":         nadam.config.Beta2,
                "epsilon":       nadam.config.Epsilon,
                "weight_decay":  nadam.config.WeightDecay,
        }
}</span>

// Cleanup releases all GPU buffers
func (nadam *NadamOptimizerState) Cleanup() <span class="cov0" title="0">{
        for i := range nadam.momentumBuffers </span><span class="cov0" title="0">{
                if nadam.momentumBuffers[i] != nil </span><span class="cov0" title="0">{
                        nadam.memoryManager.ReleaseBuffer(nadam.momentumBuffers[i])
                        nadam.momentumBuffers[i] = nil
                }</span>
                <span class="cov0" title="0">if nadam.varianceBuffers[i] != nil </span><span class="cov0" title="0">{
                        nadam.memoryManager.ReleaseBuffer(nadam.varianceBuffers[i])
                        nadam.varianceBuffers[i] = nil
                }</span>
        }

        // Clear slices
        <span class="cov0" title="0">nadam.momentumBuffers = nil
        nadam.varianceBuffers = nil
        nadam.WeightBuffers = nil
        nadam.bufferSizes = nil</span>
}</pre>
		
		<pre class="file" id="file5" style="display: none">package optimizer

import (
        "testing"
        "math"
)

// TestDefaultConfigurations tests all optimizer default configurations
func TestDefaultConfigurations(t *testing.T) <span class="cov0" title="0">{
        t.Run("AdamConfig", func(t *testing.T) </span><span class="cov0" title="0">{
                config := DefaultAdamConfig()
                
                if config.LearningRate != 0.001 </span><span class="cov0" title="0">{
                        t.Errorf("Expected learning rate 0.001, got %f", config.LearningRate)
                }</span>
                
                <span class="cov0" title="0">if config.Beta1 != 0.9 </span><span class="cov0" title="0">{
                        t.Errorf("Expected beta1 0.9, got %f", config.Beta1)
                }</span>
                
                <span class="cov0" title="0">if config.Beta2 != 0.999 </span><span class="cov0" title="0">{
                        t.Errorf("Expected beta2 0.999, got %f", config.Beta2)
                }</span>
                
                <span class="cov0" title="0">if config.Epsilon != 1e-8 </span><span class="cov0" title="0">{
                        t.Errorf("Expected epsilon 1e-8, got %f", config.Epsilon)
                }</span>
                
                <span class="cov0" title="0">if config.WeightDecay != 0.0 </span><span class="cov0" title="0">{
                        t.Errorf("Expected weight decay 0.0, got %f", config.WeightDecay)
                }</span>
        })
        
        <span class="cov0" title="0">t.Run("AdaDeltaConfig", func(t *testing.T) </span><span class="cov0" title="0">{
                config := DefaultAdaDeltaConfig()
                
                // AdaDelta doesn't have a fixed learning rate
                
                if config.Rho != 0.95 </span><span class="cov0" title="0">{
                        t.Errorf("Expected rho 0.95, got %f", config.Rho)
                }</span>
                
                <span class="cov0" title="0">if config.Epsilon != 1e-6 </span><span class="cov0" title="0">{
                        t.Errorf("Expected epsilon 1e-6, got %f", config.Epsilon)
                }</span>
                
                <span class="cov0" title="0">if config.WeightDecay != 0.0 </span><span class="cov0" title="0">{
                        t.Errorf("Expected weight decay 0.0, got %f", config.WeightDecay)
                }</span>
        })
        
        <span class="cov0" title="0">t.Run("AdaGradConfig", func(t *testing.T) </span><span class="cov0" title="0">{
                config := DefaultAdaGradConfig()
                
                if config.LearningRate != 0.01 </span><span class="cov0" title="0">{
                        t.Errorf("Expected learning rate 0.01, got %f", config.LearningRate)
                }</span>
                
                <span class="cov0" title="0">if config.Epsilon != 1e-10 </span><span class="cov0" title="0">{
                        t.Errorf("Expected epsilon 1e-10, got %f", config.Epsilon)
                }</span>
                
                <span class="cov0" title="0">if config.WeightDecay != 0.0 </span><span class="cov0" title="0">{
                        t.Errorf("Expected weight decay 0.0, got %f", config.WeightDecay)
                }</span>
        })
        
        <span class="cov0" title="0">t.Run("LBFGSConfig", func(t *testing.T) </span><span class="cov0" title="0">{
                config := DefaultLBFGSConfig()
                
                if config.HistorySize != 10 </span><span class="cov0" title="0">{
                        t.Errorf("Expected history size 10, got %d", config.HistorySize)
                }</span>
                
                <span class="cov0" title="0">if config.LineSearchTol != 1e-4 </span><span class="cov0" title="0">{
                        t.Errorf("Expected line search tolerance 1e-4, got %f", config.LineSearchTol)
                }</span>
                
                <span class="cov0" title="0">if config.MaxLineSearch != 20 </span><span class="cov0" title="0">{
                        t.Errorf("Expected max line search 20, got %d", config.MaxLineSearch)
                }</span>
                
                <span class="cov0" title="0">if config.C1 != 1e-4 </span><span class="cov0" title="0">{
                        t.Errorf("Expected C1 1e-4, got %f", config.C1)
                }</span>
                
                <span class="cov0" title="0">if config.C2 != 0.9 </span><span class="cov0" title="0">{
                        t.Errorf("Expected C2 0.9, got %f", config.C2)
                }</span>
                
                <span class="cov0" title="0">if config.InitialStep != 1.0 </span><span class="cov0" title="0">{
                        t.Errorf("Expected initial step 1.0, got %f", config.InitialStep)
                }</span>
        })
        
        <span class="cov0" title="0">t.Run("RMSPropConfig", func(t *testing.T) </span><span class="cov0" title="0">{
                config := DefaultRMSPropConfig()
                
                if config.LearningRate != 0.01 </span><span class="cov0" title="0">{
                        t.Errorf("Expected learning rate 0.01, got %f", config.LearningRate)
                }</span>
                
                <span class="cov0" title="0">if config.Alpha != 0.99 </span><span class="cov0" title="0">{
                        t.Errorf("Expected alpha 0.99, got %f", config.Alpha)
                }</span>
                
                <span class="cov0" title="0">if config.Epsilon != 1e-8 </span><span class="cov0" title="0">{
                        t.Errorf("Expected epsilon 1e-8, got %f", config.Epsilon)
                }</span>
                
                <span class="cov0" title="0">if config.WeightDecay != 0.0 </span><span class="cov0" title="0">{
                        t.Errorf("Expected weight decay 0.0, got %f", config.WeightDecay)
                }</span>
                
                <span class="cov0" title="0">if config.Momentum != 0.0 </span><span class="cov0" title="0">{
                        t.Errorf("Expected momentum 0.0, got %f", config.Momentum)
                }</span>
                
                <span class="cov0" title="0">if config.Centered != false </span><span class="cov0" title="0">{
                        t.Errorf("Expected centered false, got %t", config.Centered)
                }</span>
        })
        
        <span class="cov0" title="0">t.Run("NADAMConfig", func(t *testing.T) </span><span class="cov0" title="0">{
                config := DefaultNadamConfig()
                
                if config.LearningRate != 0.002 </span><span class="cov0" title="0">{
                        t.Errorf("Expected learning rate 0.002, got %f", config.LearningRate)
                }</span>
                
                <span class="cov0" title="0">if config.Beta1 != 0.9 </span><span class="cov0" title="0">{
                        t.Errorf("Expected beta1 0.9, got %f", config.Beta1)
                }</span>
                
                <span class="cov0" title="0">if config.Beta2 != 0.999 </span><span class="cov0" title="0">{
                        t.Errorf("Expected beta2 0.999, got %f", config.Beta2)
                }</span>
                
                <span class="cov0" title="0">if config.Epsilon != 1e-8 </span><span class="cov0" title="0">{
                        t.Errorf("Expected epsilon 1e-8, got %f", config.Epsilon)
                }</span>
                
                <span class="cov0" title="0">if config.WeightDecay != 0.0 </span><span class="cov0" title="0">{
                        t.Errorf("Expected weight decay 0.0, got %f", config.WeightDecay)
                }</span>
                
                // NADAM doesn't have ScheduleDecay in this implementation
        })
        
        <span class="cov0" title="0">t.Log("All default configuration tests passed")</span>
}

// TestTensorSizeCalculation tests tensor size calculation utilities
func TestTensorSizeCalculation(t *testing.T) <span class="cov0" title="0">{
        tests := []struct {
                name     string
                shape    []int
                expected int
        }{
                {"scalar", []int{1}, 1},
                {"vector", []int{10}, 10},
                {"matrix", []int{3, 4}, 12},
                {"tensor_3d", []int{2, 3, 4}, 24},
                {"tensor_4d", []int{2, 3, 4, 5}, 120},
                {"empty", []int{}, 0},
                {"zero_dim", []int{0}, 0},
                {"mixed", []int{5, 0, 3}, 0},
        }
        
        for _, test := range tests </span><span class="cov0" title="0">{
                t.Run(test.name, func(t *testing.T) </span><span class="cov0" title="0">{
                        result := calculateTensorSize(test.shape)
                        if result != test.expected </span><span class="cov0" title="0">{
                                t.Errorf("calculateTensorSize(%v) = %d; expected %d", 
                                        test.shape, result, test.expected)
                        }</span>
                })
        }
        
        <span class="cov0" title="0">t.Log("Tensor size calculation tests passed")</span>
}

// TestOptimizerConfigValidation tests configuration validation
func TestOptimizerConfigValidation(t *testing.T) <span class="cov0" title="0">{
        t.Run("AdamConfigValidation", func(t *testing.T) </span><span class="cov0" title="0">{
                // Test valid configuration
                validConfig := AdamConfig{
                        LearningRate: 0.001,
                        Beta1:        0.9,
                        Beta2:        0.999,
                        Epsilon:      1e-8,
                        WeightDecay:  0.0,
                }
                
                // Validate ranges
                if validConfig.LearningRate &lt;= 0 </span><span class="cov0" title="0">{
                        t.Error("Learning rate should be positive")
                }</span>
                
                <span class="cov0" title="0">if validConfig.Beta1 &lt; 0 || validConfig.Beta1 &gt;= 1 </span><span class="cov0" title="0">{
                        t.Error("Beta1 should be in [0, 1)")
                }</span>
                
                <span class="cov0" title="0">if validConfig.Beta2 &lt; 0 || validConfig.Beta2 &gt;= 1 </span><span class="cov0" title="0">{
                        t.Error("Beta2 should be in [0, 1)")
                }</span>
                
                <span class="cov0" title="0">if validConfig.Epsilon &lt;= 0 </span><span class="cov0" title="0">{
                        t.Error("Epsilon should be positive")
                }</span>
                
                <span class="cov0" title="0">if validConfig.WeightDecay &lt; 0 </span><span class="cov0" title="0">{
                        t.Error("Weight decay should be non-negative")
                }</span>
        })
        
        <span class="cov0" title="0">t.Run("LBFGSConfigValidation", func(t *testing.T) </span><span class="cov0" title="0">{
                validConfig := LBFGSConfig{
                        HistorySize:    10,
                        LineSearchTol:  1e-4,
                        MaxLineSearch:  20,
                        C1:             1e-4,
                        C2:             0.9,
                        InitialStep:    1.0,
                }
                
                if validConfig.HistorySize &lt;= 0 </span><span class="cov0" title="0">{
                        t.Error("History size should be positive")
                }</span>
                
                <span class="cov0" title="0">if validConfig.LineSearchTol &lt;= 0 </span><span class="cov0" title="0">{
                        t.Error("Line search tolerance should be positive")
                }</span>
                
                <span class="cov0" title="0">if validConfig.MaxLineSearch &lt;= 0 </span><span class="cov0" title="0">{
                        t.Error("Max line search should be positive")
                }</span>
                
                <span class="cov0" title="0">if validConfig.C1 &lt;= 0 || validConfig.C1 &gt;= validConfig.C2 </span><span class="cov0" title="0">{
                        t.Error("C1 should be positive and less than C2")
                }</span>
                
                <span class="cov0" title="0">if validConfig.C2 &lt;= validConfig.C1 || validConfig.C2 &gt;= 1.0 </span><span class="cov0" title="0">{
                        t.Error("C2 should be greater than C1 and less than 1.0")
                }</span>
                
                <span class="cov0" title="0">if validConfig.InitialStep &lt;= 0 </span><span class="cov0" title="0">{
                        t.Error("Initial step should be positive")
                }</span>
        })
        
        <span class="cov0" title="0">t.Log("Configuration validation tests passed")</span>
}

// TestOptimizerDataStructures tests basic data structures
func TestOptimizerDataStructures(t *testing.T) <span class="cov0" title="0">{
        t.Run("AdamStats", func(t *testing.T) </span><span class="cov0" title="0">{
                stats := AdamStats{
                        StepCount:       100,
                        LearningRate:    0.001,
                        Beta1:           0.9,
                        Beta2:           0.999,
                        Epsilon:         1e-8,
                        WeightDecay:     0.0,
                        NumParameters:   2,
                        TotalBufferSize: 1024,
                }
                
                if stats.StepCount != 100 </span><span class="cov0" title="0">{
                        t.Errorf("Expected step count 100, got %d", stats.StepCount)
                }</span>
                
                <span class="cov0" title="0">if stats.LearningRate != 0.001 </span><span class="cov0" title="0">{
                        t.Errorf("Expected learning rate 0.001, got %f", stats.LearningRate)
                }</span>
                
                <span class="cov0" title="0">if stats.NumParameters != 2 </span><span class="cov0" title="0">{
                        t.Errorf("Expected 2 parameters, got %d", stats.NumParameters)
                }</span>
        })
        
        <span class="cov0" title="0">t.Log("Data structure tests passed")</span>
}

// TestOptimizerMathUtilities tests mathematical utility functions
func TestOptimizerMathUtilities(t *testing.T) <span class="cov0" title="0">{
        t.Run("NumericalStability", func(t *testing.T) </span><span class="cov0" title="0">{
                // Test various numerical edge cases
                testValues := []float32{
                        0.0,
                        1e-10, // Very small positive
                        1e10,  // Very large
                        math.MaxFloat32,
                        math.SmallestNonzeroFloat32,
                }
                
                for _, val := range testValues </span><span class="cov0" title="0">{
                        if math.IsNaN(float64(val)) </span><span class="cov0" title="0">{
                                t.Errorf("Value should not be NaN: %f", val)
                        }</span>
                        <span class="cov0" title="0">if math.IsInf(float64(val), 0) &amp;&amp; val != math.MaxFloat32 </span><span class="cov0" title="0">{
                                t.Errorf("Unexpected infinite value: %f", val)
                        }</span>
                }
        })
        
        <span class="cov0" title="0">t.Run("EpsilonHandling", func(t *testing.T) </span><span class="cov0" title="0">{
                epsilon := float32(1e-8)
                testVal := float32(0.0)
                
                // Test that epsilon prevents division by zero
                result := testVal + epsilon
                if result &lt;= 0 </span><span class="cov0" title="0">{
                        t.Error("Epsilon should prevent zero or negative values")
                }</span>
                
                // Test that epsilon is small enough not to affect normal values
                <span class="cov0" title="0">normalVal := float32(1.0)
                if (normalVal+epsilon)/normalVal-1.0 &gt; 1e-6 </span><span class="cov0" title="0">{
                        t.Error("Epsilon should be small relative to normal values")
                }</span>
        })
        
        <span class="cov0" title="0">t.Log("Math utility tests passed")</span>
}

// TestOptimizerErrorHandling tests error handling scenarios
func TestOptimizerErrorHandling(t *testing.T) <span class="cov0" title="0">{
        t.Run("InvalidShapes", func(t *testing.T) </span><span class="cov0" title="0">{
                // Test empty shapes
                emptyShapes := [][]int{}
                size := calculateTensorSize([]int{})
                if size != 0 </span><span class="cov0" title="0">{
                        t.Errorf("Expected size 0 for empty shape, got %d", size)
                }</span>
                
                // Test negative dimensions
                <span class="cov0" title="0">negativeShape := []int{-1, 5}
                size = calculateTensorSize(negativeShape)
                if size &gt;= 0 </span><span class="cov0" title="0">{
                        t.Error("Expected negative or zero size for invalid shape")
                }</span>
                
                // Test shapes with zero dimensions
                <span class="cov0" title="0">zeroShape := []int{5, 0, 3}
                size = calculateTensorSize(zeroShape)
                if size != 0 </span><span class="cov0" title="0">{
                        t.Errorf("Expected size 0 for shape with zero dimension, got %d", size)
                }</span>
                
                <span class="cov0" title="0">_ = emptyShapes</span> // Use variable to avoid compiler warning
        })
        
        <span class="cov0" title="0">t.Run("ConfigurationEdgeCases", func(t *testing.T) </span><span class="cov0" title="0">{
                // Test extreme configuration values
                extremeConfig := AdamConfig{
                        LearningRate: math.MaxFloat32,
                        Beta1:        0.999999,
                        Beta2:        0.999999,
                        Epsilon:      math.SmallestNonzeroFloat32,
                        WeightDecay:  math.MaxFloat32,
                }
                
                // These should be valid but may cause numerical issues
                if extremeConfig.LearningRate &lt;= 0 </span><span class="cov0" title="0">{
                        t.Error("Learning rate should be positive even at extreme values")
                }</span>
                
                <span class="cov0" title="0">if extremeConfig.Beta1 &gt;= 1.0 </span><span class="cov0" title="0">{
                        t.Error("Beta1 should be less than 1.0")
                }</span>
        })
        
        <span class="cov0" title="0">t.Log("Error handling tests passed")</span>
}

// TestOptimizerStateConcepts tests optimizer state management concepts
func TestOptimizerStateConcepts(t *testing.T) <span class="cov0" title="0">{
        t.Run("AdamStateConcept", func(t *testing.T) </span><span class="cov0" title="0">{
                // Test Adam state concept - momentum and velocity tracking
                // This tests the conceptual understanding without Metal allocation
                
                numParams := 3
                shapes := [][]int{
                        {10, 5},  // Weight matrix
                        {5},      // Bias vector
                        {5, 1},   // Output weight
                }
                
                // Verify we can calculate state requirements
                totalParams := 0
                for _, shape := range shapes </span><span class="cov0" title="0">{
                        totalParams += calculateTensorSize(shape)
                }</span>
                
                <span class="cov0" title="0">expectedParams := 50 + 5 + 5 // 10*5 + 5 + 5*1
                if totalParams != expectedParams </span><span class="cov0" title="0">{
                        t.Errorf("Expected %d total parameters, got %d", expectedParams, totalParams)
                }</span>
                
                // Adam needs 2 state tensors per parameter tensor (momentum and velocity)
                <span class="cov0" title="0">expectedStateTensors := numParams * 2
                if expectedStateTensors != 6 </span><span class="cov0" title="0">{
                        t.Errorf("Expected 6 state tensors for Adam, got %d", expectedStateTensors)
                }</span>
                
                // L-BFGS needs history_size * num_params gradients + history_size * num_params parameter changes
                <span class="cov0" title="0">historySize := 10
                expectedLBFGSState := historySize * numParams * 2
                if expectedLBFGSState != 60 </span><span class="cov0" title="0">{
                        t.Errorf("Expected 60 state tensors for L-BFGS, got %d", expectedLBFGSState)
                }</span>
        })
        
        <span class="cov0" title="0">t.Run("OptimizerMemoryEstimation", func(t *testing.T) </span><span class="cov0" title="0">{
                // Test memory estimation concepts
                shapes := [][]int{{1000, 1000}} // 1M parameters
                
                paramCount := calculateTensorSize(shapes[0])
                bytesPerFloat32 := 4
                
                // Parameter memory
                paramMemory := paramCount * bytesPerFloat32
                
                // Adam state memory (momentum + velocity)
                adamStateMemory := paramMemory * 2
                
                // Total Adam memory
                adamTotalMemory := paramMemory + adamStateMemory
                
                expectedAdamMemory := 1000000 * 4 * 3 // 12MB
                if adamTotalMemory != expectedAdamMemory </span><span class="cov0" title="0">{
                        t.Errorf("Expected Adam memory %d, got %d", expectedAdamMemory, adamTotalMemory)
                }</span>
                
                // AdaGrad state memory (accumulated gradients)
                <span class="cov0" title="0">adagradStateMemory := paramMemory * 1
                adagradTotalMemory := paramMemory + adagradStateMemory
                
                expectedAdagradMemory := 1000000 * 4 * 2 // 8MB
                if adagradTotalMemory != expectedAdagradMemory </span><span class="cov0" title="0">{
                        t.Errorf("Expected AdaGrad memory %d, got %d", expectedAdagradMemory, adagradTotalMemory)
                }</span>
        })
        
        <span class="cov0" title="0">t.Log("Optimizer state concept tests passed")</span>
}

// TestOptimizerAlgorithmConcepts tests algorithm-specific concepts
func TestOptimizerAlgorithmConcepts(t *testing.T) <span class="cov0" title="0">{
        t.Run("AdamBiasCorrection", func(t *testing.T) </span><span class="cov0" title="0">{
                // Test bias correction factors for Adam
                beta1 := float32(0.9)
                beta2 := float32(0.999)
                
                // Test first few iterations
                for step := 1; step &lt;= 10; step++ </span><span class="cov0" title="0">{
                        beta1_t := float32(math.Pow(float64(beta1), float64(step)))
                        beta2_t := float32(math.Pow(float64(beta2), float64(step)))
                        
                        bias1_correction := 1.0 - beta1_t
                        bias2_correction := 1.0 - beta2_t
                        
                        if bias1_correction &lt;= 0 || bias1_correction &gt; 1 </span><span class="cov0" title="0">{
                                t.Errorf("Invalid bias1 correction at step %d: %f", step, bias1_correction)
                        }</span>
                        
                        <span class="cov0" title="0">if bias2_correction &lt;= 0 || bias2_correction &gt; 1 </span><span class="cov0" title="0">{
                                t.Errorf("Invalid bias2 correction at step %d: %f", step, bias2_correction)
                        }</span>
                        
                        // Bias correction should approach 1 as step increases
                        <span class="cov0" title="0">if step &gt; 1 </span><span class="cov0" title="0">{
                                if bias1_correction &lt;= float32(math.Pow(float64(beta1), float64(step-1))) </span><span class="cov0" title="0">{
                                        t.Errorf("Bias1 correction should increase with iterations")
                                }</span>
                        }
                }
        })
        
        <span class="cov0" title="0">t.Run("LearningRateScheduling", func(t *testing.T) </span><span class="cov0" title="0">{
                // Test learning rate scheduling concepts
                initialLR := float32(0.001)
                
                // Linear decay
                for step := 1; step &lt;= 100; step++ </span><span class="cov0" title="0">{
                        decayedLR := initialLR * (1.0 - float32(step)/100.0)
                        if decayedLR &lt; 0 </span><span class="cov0" title="0">{
                                t.Errorf("Decayed learning rate should not be negative at step %d", step)
                        }</span>
                        <span class="cov0" title="0">if step == 100 &amp;&amp; decayedLR != 0.0 </span><span class="cov0" title="0">{
                                t.Errorf("Learning rate should reach 0 at final step, got %f", decayedLR)
                        }</span>
                }
                
                // Exponential decay
                <span class="cov0" title="0">decayRate := float32(0.95)
                for step := 1; step &lt;= 10; step++ </span><span class="cov0" title="0">{
                        decayedLR := initialLR * float32(math.Pow(float64(decayRate), float64(step)))
                        if decayedLR &lt;= 0 || decayedLR &gt; initialLR </span><span class="cov0" title="0">{
                                t.Errorf("Invalid exponential decay at step %d: %f", step, decayedLR)
                        }</span>
                }
        })
        
        <span class="cov0" title="0">t.Log("Algorithm concept tests passed")</span>
}

// TestOptimizerPerformanceConcepts tests performance-related concepts
func TestOptimizerPerformanceConcepts(t *testing.T) <span class="cov0" title="0">{
        t.Run("BatchSizeEffects", func(t *testing.T) </span><span class="cov0" title="0">{
                // Test how batch size affects optimizer behavior conceptually
                batchSizes := []int{1, 32, 128, 512}
                
                for _, batchSize := range batchSizes </span><span class="cov0" title="0">{
                        // Larger batch sizes generally provide more stable gradients
                        // but require more memory
                        memoryMultiplier := float32(batchSize)
                        gradientStability := 1.0 / float32(math.Sqrt(float64(batchSize)))
                        
                        if memoryMultiplier &lt;= 0 </span><span class="cov0" title="0">{
                                t.Errorf("Memory multiplier should be positive for batch size %d", batchSize)
                        }</span>
                        
                        <span class="cov0" title="0">if gradientStability &lt;= 0 || gradientStability &gt; 1.0 </span><span class="cov0" title="0">{
                                t.Errorf("Invalid gradient stability for batch size %d: %f", batchSize, gradientStability)
                        }</span>
                }
        })
        
        <span class="cov0" title="0">t.Run("ConvergenceRates", func(t *testing.T) </span><span class="cov0" title="0">{
                // Test convergence rate concepts for different optimizers
                
                // Adam typically converges faster than SGD due to adaptive learning rates
                adamLR := float32(0.001)
                sgdLR := float32(0.01) // Usually needs higher learning rate
                
                if adamLR &gt;= sgdLR </span><span class="cov0" title="0">{
                        t.Error("Adam typically uses lower learning rates than SGD")
                }</span>
                
                // AdaGrad accumulates all gradients, so learning rate decreases over time
                <span class="cov0" title="0">adagradAccumulator := float32(0.0)
                epsilon := float32(1e-8)
                
                // Simulate gradient accumulation
                for i := 0; i &lt; 10; i++ </span><span class="cov0" title="0">{
                        gradient := float32(0.1)
                        adagradAccumulator += gradient * gradient
                        effectiveLR := sgdLR / float32(math.Sqrt(float64(adagradAccumulator+epsilon)))
                        
                        if effectiveLR &lt;= 0 </span><span class="cov0" title="0">{
                                t.Errorf("Effective learning rate should be positive at step %d", i)
                        }</span>
                        
                        <span class="cov0" title="0">if i &gt; 0 &amp;&amp; effectiveLR &gt;= sgdLR </span><span class="cov0" title="0">{
                                t.Error("AdaGrad effective learning rate should decrease over time")
                        }</span>
                }
        })
        
        <span class="cov0" title="0">t.Log("Performance concept tests passed")</span>
}

// Helper function - uses the existing calculateTensorSize from adam.go
// Note: Removed duplicate function definition

// Benchmarks for performance testing

// BenchmarkTensorSizeCalculation benchmarks tensor size calculation
func BenchmarkTensorSizeCalculation(b *testing.B) <span class="cov0" title="0">{
        shapes := [][]int{
                {10},
                {10, 10},
                {10, 10, 10},
                {10, 10, 10, 10},
                {100, 100},
                {1000},
        }
        
        b.ResetTimer()
        for i := 0; i &lt; b.N; i++ </span><span class="cov0" title="0">{
                shape := shapes[i%len(shapes)]
                calculateTensorSize(shape)
        }</span>
}

// BenchmarkConfigurationCreation benchmarks config creation
func BenchmarkConfigurationCreation(b *testing.B) <span class="cov0" title="0">{
        b.ResetTimer()
        for i := 0; i &lt; b.N; i++ </span><span class="cov0" title="0">{
                _ = DefaultAdamConfig()
                _ = DefaultLBFGSConfig()
                _ = DefaultAdaGradConfig()
                _ = DefaultRMSPropConfig()
        }</span>
}

// BenchmarkBiasCorrection benchmarks Adam bias correction calculation
func BenchmarkBiasCorrection(b *testing.B) <span class="cov0" title="0">{
        beta1 := float32(0.9)
        beta2 := float32(0.999)
        
        b.ResetTimer()
        for i := 0; i &lt; b.N; i++ </span><span class="cov0" title="0">{
                step := float64(i%1000 + 1)
                beta1_t := float32(math.Pow(float64(beta1), step))
                beta2_t := float32(math.Pow(float64(beta2), step))
                _ = 1.0 - beta1_t
                _ = 1.0 - beta2_t
        }</span>
}</pre>
		
		<pre class="file" id="file6" style="display: none">package optimizer

import (
        "fmt"
        "unsafe"

        "github.com/tsawler/go-metal/cgo_bridge"
        "github.com/tsawler/go-metal/memory"
)

// RMSPropOptimizerState represents GPU-resident RMSProp optimizer state
type RMSPropOptimizerState struct {
        // Hyperparameters
        LearningRate float32
        Alpha        float32 // Smoothing constant (typically 0.99)
        Epsilon      float32 // Small constant to prevent division by zero (typically 1e-8)
        WeightDecay  float32 // L2 regularization coefficient
        Momentum     float32 // Momentum coefficient (typically 0.9, 0.0 for no momentum)
        Centered     bool    // Whether to use centered RMSProp (subtract mean of gradients)

        // GPU-resident state buffers
        SquaredGradAvgBuffers []unsafe.Pointer // Running average of squared gradients for each weight tensor
        MomentumBuffers       []unsafe.Pointer // Momentum buffers for each weight tensor (if momentum &gt; 0)
        GradientAvgBuffers    []unsafe.Pointer // Running average of gradients for each weight tensor (if centered)
        WeightBuffers         []unsafe.Pointer // Current weight tensors

        // Step tracking
        StepCount uint64

        // Buffer management
        memoryManager *memory.MemoryManager
        device        unsafe.Pointer

        // Buffer sizes for proper cleanup
        bufferSizes []int

        // Command buffer pooling
        commandPool unsafe.Pointer // Optional command buffer pool for Metal operations
        usePooling  bool           // Whether to use command buffer pooling
}

// RMSPropConfig holds configuration for RMSProp optimizer
type RMSPropConfig struct {
        LearningRate float32
        Alpha        float32
        Epsilon      float32
        WeightDecay  float32
        Momentum     float32
        Centered     bool
}

// DefaultRMSPropConfig returns default RMSProp optimizer configuration
func DefaultRMSPropConfig() RMSPropConfig <span class="cov8" title="1">{
        return RMSPropConfig{
                LearningRate: 0.01,
                Alpha:        0.99,
                Epsilon:      1e-8,
                WeightDecay:  0.0,
                Momentum:     0.0,
                Centered:     false,
        }
}</span>

// NewRMSPropOptimizer creates a new GPU-resident RMSProp optimizer
func NewRMSPropOptimizer(
        config RMSPropConfig,
        weightShapes [][]int,
        memoryManager *memory.MemoryManager,
        device unsafe.Pointer,
) (*RMSPropOptimizerState, error) <span class="cov0" title="0">{
        if memoryManager == nil </span><span class="cov0" title="0">{
                return nil, fmt.Errorf("memory manager cannot be nil")
        }</span>

        <span class="cov0" title="0">if device == nil </span><span class="cov0" title="0">{
                return nil, fmt.Errorf("device cannot be nil")
        }</span>

        <span class="cov0" title="0">if len(weightShapes) == 0 </span><span class="cov0" title="0">{
                return nil, fmt.Errorf("no weight shapes provided")
        }</span>

        <span class="cov0" title="0">numWeights := len(weightShapes)

        rmsprop := &amp;RMSPropOptimizerState{
                LearningRate:          config.LearningRate,
                Alpha:                 config.Alpha,
                Epsilon:               config.Epsilon,
                WeightDecay:           config.WeightDecay,
                Momentum:              config.Momentum,
                Centered:              config.Centered,
                SquaredGradAvgBuffers: make([]unsafe.Pointer, numWeights),
                MomentumBuffers:       make([]unsafe.Pointer, numWeights),
                GradientAvgBuffers:    make([]unsafe.Pointer, numWeights),
                WeightBuffers:         make([]unsafe.Pointer, numWeights),
                StepCount:             0,
                memoryManager:         memoryManager,
                device:                device,
                bufferSizes:           make([]int, numWeights),
        }

        // Allocate GPU buffers for squared gradient averages (always needed)
        for i, shape := range weightShapes </span><span class="cov0" title="0">{
                // Calculate buffer size (assume float32)
                size := calculateTensorSize(shape) * 4 // 4 bytes per float32
                rmsprop.bufferSizes[i] = size

                // Allocate squared gradient average buffer
                squaredGradAvgBuffer := rmsprop.memoryManager.AllocateBuffer(size)
                if squaredGradAvgBuffer == nil </span><span class="cov0" title="0">{
                        rmsprop.cleanup(i) // Cleanup previously allocated buffers
                        return nil, fmt.Errorf("failed to allocate squared gradient average buffer for weight %d", i)
                }</span>
                <span class="cov0" title="0">rmsprop.SquaredGradAvgBuffers[i] = squaredGradAvgBuffer

                // Allocate momentum buffer if momentum &gt; 0
                if config.Momentum &gt; 0.0 </span><span class="cov0" title="0">{
                        momentumBuffer := rmsprop.memoryManager.AllocateBuffer(size)
                        if momentumBuffer == nil </span><span class="cov0" title="0">{
                                rmsprop.memoryManager.ReleaseBuffer(squaredGradAvgBuffer)
                                rmsprop.cleanup(i)
                                return nil, fmt.Errorf("failed to allocate momentum buffer for weight %d", i)
                        }</span>
                        <span class="cov0" title="0">rmsprop.MomentumBuffers[i] = momentumBuffer</span>
                }

                // Allocate gradient average buffer if centered
                <span class="cov0" title="0">if config.Centered </span><span class="cov0" title="0">{
                        gradientAvgBuffer := rmsprop.memoryManager.AllocateBuffer(size)
                        if gradientAvgBuffer == nil </span><span class="cov0" title="0">{
                                rmsprop.memoryManager.ReleaseBuffer(squaredGradAvgBuffer)
                                if rmsprop.MomentumBuffers[i] != nil </span><span class="cov0" title="0">{
                                        rmsprop.memoryManager.ReleaseBuffer(rmsprop.MomentumBuffers[i])
                                }</span>
                                <span class="cov0" title="0">rmsprop.cleanup(i)
                                return nil, fmt.Errorf("failed to allocate gradient average buffer for weight %d", i)</span>
                        }
                        <span class="cov0" title="0">rmsprop.GradientAvgBuffers[i] = gradientAvgBuffer</span>
                }

                // Initialize buffers to zero
                <span class="cov0" title="0">err := cgo_bridge.ZeroMetalBuffer(rmsprop.device, squaredGradAvgBuffer, size)
                if err != nil </span><span class="cov0" title="0">{
                        rmsprop.memoryManager.ReleaseBuffer(squaredGradAvgBuffer)
                        if rmsprop.MomentumBuffers[i] != nil </span><span class="cov0" title="0">{
                                rmsprop.memoryManager.ReleaseBuffer(rmsprop.MomentumBuffers[i])
                        }</span>
                        <span class="cov0" title="0">if rmsprop.GradientAvgBuffers[i] != nil </span><span class="cov0" title="0">{
                                rmsprop.memoryManager.ReleaseBuffer(rmsprop.GradientAvgBuffers[i])
                        }</span>
                        <span class="cov0" title="0">rmsprop.cleanup(i)
                        return nil, fmt.Errorf("failed to zero squared gradient average buffer for weight %d: %v", i, err)</span>
                }

                <span class="cov0" title="0">if rmsprop.MomentumBuffers[i] != nil </span><span class="cov0" title="0">{
                        err = cgo_bridge.ZeroMetalBuffer(rmsprop.device, rmsprop.MomentumBuffers[i], size)
                        if err != nil </span><span class="cov0" title="0">{
                                rmsprop.memoryManager.ReleaseBuffer(squaredGradAvgBuffer)
                                rmsprop.memoryManager.ReleaseBuffer(rmsprop.MomentumBuffers[i])
                                if rmsprop.GradientAvgBuffers[i] != nil </span><span class="cov0" title="0">{
                                        rmsprop.memoryManager.ReleaseBuffer(rmsprop.GradientAvgBuffers[i])
                                }</span>
                                <span class="cov0" title="0">rmsprop.cleanup(i)
                                return nil, fmt.Errorf("failed to zero momentum buffer for weight %d: %v", i, err)</span>
                        }
                }

                <span class="cov0" title="0">if rmsprop.GradientAvgBuffers[i] != nil </span><span class="cov0" title="0">{
                        err = cgo_bridge.ZeroMetalBuffer(rmsprop.device, rmsprop.GradientAvgBuffers[i], size)
                        if err != nil </span><span class="cov0" title="0">{
                                rmsprop.memoryManager.ReleaseBuffer(squaredGradAvgBuffer)
                                if rmsprop.MomentumBuffers[i] != nil </span><span class="cov0" title="0">{
                                        rmsprop.memoryManager.ReleaseBuffer(rmsprop.MomentumBuffers[i])
                                }</span>
                                <span class="cov0" title="0">rmsprop.memoryManager.ReleaseBuffer(rmsprop.GradientAvgBuffers[i])
                                rmsprop.cleanup(i)
                                return nil, fmt.Errorf("failed to zero gradient average buffer for weight %d: %v", i, err)</span>
                        }
                }
        }

        <span class="cov0" title="0">return rmsprop, nil</span>
}

// cleanup releases previously allocated buffers in case of partial initialization failure
func (rmsprop *RMSPropOptimizerState) cleanup(upToIndex int) <span class="cov0" title="0">{
        for i := 0; i &lt; upToIndex; i++ </span><span class="cov0" title="0">{
                if rmsprop.SquaredGradAvgBuffers[i] != nil </span><span class="cov0" title="0">{
                        rmsprop.memoryManager.ReleaseBuffer(rmsprop.SquaredGradAvgBuffers[i])
                        rmsprop.SquaredGradAvgBuffers[i] = nil
                }</span>
                <span class="cov0" title="0">if rmsprop.MomentumBuffers[i] != nil </span><span class="cov0" title="0">{
                        rmsprop.memoryManager.ReleaseBuffer(rmsprop.MomentumBuffers[i])
                        rmsprop.MomentumBuffers[i] = nil
                }</span>
                <span class="cov0" title="0">if rmsprop.GradientAvgBuffers[i] != nil </span><span class="cov0" title="0">{
                        rmsprop.memoryManager.ReleaseBuffer(rmsprop.GradientAvgBuffers[i])
                        rmsprop.GradientAvgBuffers[i] = nil
                }</span>
        }
}

// SetWeightBuffers sets the current weight buffer pointers
// This should be called before each optimization step
func (rmsprop *RMSPropOptimizerState) SetWeightBuffers(weightBuffers []unsafe.Pointer) error <span class="cov0" title="0">{
        if len(weightBuffers) != len(rmsprop.WeightBuffers) </span><span class="cov0" title="0">{
                return fmt.Errorf("expected %d weight buffers, got %d", len(rmsprop.WeightBuffers), len(weightBuffers))
        }</span>

        <span class="cov0" title="0">copy(rmsprop.WeightBuffers, weightBuffers)
        return nil</span>
}

// Step performs a single RMSProp optimization step
func (rmsprop *RMSPropOptimizerState) Step(gradientBuffers []unsafe.Pointer) error <span class="cov0" title="0">{
        if len(gradientBuffers) != len(rmsprop.WeightBuffers) </span><span class="cov0" title="0">{
                return fmt.Errorf("gradient buffers length (%d) doesn't match weight buffers length (%d)",
                        len(gradientBuffers), len(rmsprop.WeightBuffers))
        }</span>

        <span class="cov0" title="0">rmsprop.StepCount++

        var err error
        err = cgo_bridge.ExecuteRMSPropStepMPSGraph(
                rmsprop.device,
                rmsprop.WeightBuffers,
                gradientBuffers,
                rmsprop.SquaredGradAvgBuffers,
                rmsprop.MomentumBuffers,
                rmsprop.GradientAvgBuffers,
                rmsprop.bufferSizes,
                rmsprop.LearningRate,
                rmsprop.Alpha,
                rmsprop.Epsilon,
                rmsprop.WeightDecay,
                rmsprop.Momentum,
                rmsprop.Centered,
                int(rmsprop.StepCount),
        )

        if err != nil </span><span class="cov0" title="0">{
                return fmt.Errorf("RMSProp step execution failed: %v", err)
        }</span>

        <span class="cov0" title="0">return nil</span>
}

// UpdateLearningRate updates the learning rate (useful for learning rate scheduling)
func (rmsprop *RMSPropOptimizerState) UpdateLearningRate(newLR float32) <span class="cov0" title="0">{
        rmsprop.LearningRate = newLR
}</span>

// SetCommandPool enables command buffer pooling for Metal operations
func (rmsprop *RMSPropOptimizerState) SetCommandPool(commandPool unsafe.Pointer) <span class="cov0" title="0">{
        rmsprop.commandPool = commandPool
        rmsprop.usePooling = (commandPool != nil)
}</span>

// GetStep returns the current step count
func (rmsprop *RMSPropOptimizerState) GetStep() uint64 <span class="cov0" title="0">{
        return rmsprop.StepCount
}</span>

// GetStats returns optimizer statistics
func (rmsprop *RMSPropOptimizerState) GetStats() RMSPropStats <span class="cov0" title="0">{
        return RMSPropStats{
                StepCount:       rmsprop.StepCount,
                LearningRate:    rmsprop.LearningRate,
                Alpha:           rmsprop.Alpha,
                Epsilon:         rmsprop.Epsilon,
                WeightDecay:     rmsprop.WeightDecay,
                Momentum:        rmsprop.Momentum,
                Centered:        rmsprop.Centered,
                NumParameters:   len(rmsprop.WeightBuffers),
                TotalBufferSize: rmsprop.getTotalBufferSize(),
        }
}</span>

// RMSPropStats provides statistics about the RMSProp optimizer
type RMSPropStats struct {
        StepCount       uint64
        LearningRate    float32
        Alpha           float32
        Epsilon         float32
        WeightDecay     float32
        Momentum        float32
        Centered        bool
        NumParameters   int
        TotalBufferSize int
}

// getTotalBufferSize calculates total memory used by optimizer state
func (rmsprop *RMSPropOptimizerState) getTotalBufferSize() int <span class="cov0" title="0">{
        total := 0
        for _, size := range rmsprop.bufferSizes </span><span class="cov0" title="0">{
                total += size // squared gradient average buffer (always present)
                if rmsprop.Momentum &gt; 0.0 </span><span class="cov0" title="0">{
                        total += size // momentum buffer
                }</span>
                <span class="cov0" title="0">if rmsprop.Centered </span><span class="cov0" title="0">{
                        total += size // gradient average buffer
                }</span>
        }
        <span class="cov0" title="0">return total</span>
}

// Cleanup releases all GPU buffers
func (rmsprop *RMSPropOptimizerState) Cleanup() <span class="cov0" title="0">{
        for i := range rmsprop.SquaredGradAvgBuffers </span><span class="cov0" title="0">{
                if rmsprop.SquaredGradAvgBuffers[i] != nil </span><span class="cov0" title="0">{
                        rmsprop.memoryManager.ReleaseBuffer(rmsprop.SquaredGradAvgBuffers[i])
                        rmsprop.SquaredGradAvgBuffers[i] = nil
                }</span>
                <span class="cov0" title="0">if rmsprop.MomentumBuffers[i] != nil </span><span class="cov0" title="0">{
                        rmsprop.memoryManager.ReleaseBuffer(rmsprop.MomentumBuffers[i])
                        rmsprop.MomentumBuffers[i] = nil
                }</span>
                <span class="cov0" title="0">if rmsprop.GradientAvgBuffers[i] != nil </span><span class="cov0" title="0">{
                        rmsprop.memoryManager.ReleaseBuffer(rmsprop.GradientAvgBuffers[i])
                        rmsprop.GradientAvgBuffers[i] = nil
                }</span>
        }

        // Clear slices
        <span class="cov0" title="0">rmsprop.SquaredGradAvgBuffers = nil
        rmsprop.MomentumBuffers = nil
        rmsprop.GradientAvgBuffers = nil
        rmsprop.WeightBuffers = nil
        rmsprop.bufferSizes = nil</span>
}</pre>
		
		</div>
	</body>
	<script>
	(function() {
		var files = document.getElementById('files');
		var visible;
		files.addEventListener('change', onChange, false);
		function select(part) {
			if (visible)
				visible.style.display = 'none';
			visible = document.getElementById(part);
			if (!visible)
				return;
			files.value = part;
			visible.style.display = 'block';
			location.hash = part;
		}
		function onChange() {
			select(files.value);
			window.scrollTo(0, 0);
		}
		if (location.hash != "") {
			select(location.hash.substr(1));
		}
		if (!visible) {
			select("file0");
		}
	})();
	</script>
</html>
