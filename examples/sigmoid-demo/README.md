# Sigmoid Activation Function Demo

This example demonstrates Go-Metal's Sigmoid activation function implementation, showcasing its integration with the MPSGraph-centric architecture and its use in binary classification models.

## What This Demonstrates

### üî¢ Sigmoid Function Properties
- **Mathematical Definition**: œÉ(x) = 1/(1+e^(-x))
- **Output Range**: (0, 1) - always positive, bounded
- **Key Characteristics**: S-shaped curve, differentiable everywhere
- **Symmetry**: œÉ(-x) = 1 - œÉ(x)

### üéØ Use Cases
- **Binary Classification**: Final layer for probability output (0-1 range)
- **Legacy Networks**: Traditional neural networks before ReLU dominance
- **Gating Mechanisms**: LSTM forget gates and input gates
- **Probability Mapping**: Converting logits to probabilities

## Architecture Integration

### üèóÔ∏è MPSGraph Implementation
```objective-c
// Optimal Metal Performance Shaders implementation
currentTensor = [engine->graph sigmoidWithTensor:currentTensor
                                            name:[NSString stringWithFormat:@"sigmoid_%d", layerIdx]];
```

### üöÄ Architecture Compliance
- **‚úÖ GPU-Resident**: Uses MPSGraph's built-in `sigmoidWithTensor` operation
- **‚úÖ Minimal CGO**: Single CGO call per activation layer
- **‚úÖ MPSGraph-Centric**: Native MPSGraph operation with automatic differentiation
- **‚úÖ Memory Management**: No additional GPU buffer allocations

## Running the Demo

```bash
cd examples/sigmoid-demo
go run main.go
```

### Expected Output

```
=== Go-Metal Sigmoid Activation Function Demo ===
Demonstrating Sigmoid activation: œÉ(x) = 1/(1+e^(-x))
Output range: (0, 1) - ideal for binary classification

üìã Creating binary classification model with Sigmoid...
‚úÖ Model created successfully!
   Architecture: Flatten ‚Üí Dense(128) ‚Üí Sigmoid ‚Üí Dense(64) ‚Üí Sigmoid ‚Üí Dense(1) ‚Üí Sigmoid
   Total layers: 7
   Parameters: 109,057 (trainable)

üîç Layer Details:
   Layer 1: hidden1 (Dense)
   Layer 2: sigmoid1 (Sigmoid)
      ‚Üí Sigmoid activation: œÉ(x) = 1/(1+e^(-x))
      ‚Üí Output range: (0, 1)
      ‚Üí Use case: Hidden layer activation (legacy networks)
   Layer 3: hidden2 (Dense)
   Layer 4: sigmoid2 (Sigmoid)
      ‚Üí Sigmoid activation: œÉ(x) = 1/(1+e^(-x))
      ‚Üí Output range: (0, 1)
      ‚Üí Use case: Hidden layer activation (legacy networks)
   Layer 5: output (Dense)
   Layer 6: output_sigmoid (Sigmoid)
      ‚Üí Sigmoid activation: œÉ(x) = 1/(1+e^(-x))
      ‚Üí Output range: (0, 1)
      ‚Üí Use case: Binary classification probability output

üìä Sigmoid Function Properties:
   ‚Ä¢ œÉ(0) ‚âà 0.5 (symmetric around origin)
   ‚Ä¢ œÉ(+‚àû) ‚Üí 1.0 (positive saturation)
   ‚Ä¢ œÉ(-‚àû) ‚Üí 0.0 (negative saturation)
   ‚Ä¢ Derivative: œÉ'(x) = œÉ(x) * (1 - œÉ(x))
   ‚Ä¢ Maximum gradient at x=0: œÉ'(0) = 0.25

üéâ Sigmoid activation function implementation completed!

üöÄ Ready for production use in binary classification models!
```

## Mathematical Properties

### üßÆ Function Characteristics
- **Domain**: (-‚àû, ‚àû) - accepts any real input
- **Range**: (0, 1) - always positive, never exactly 0 or 1
- **Monotonic**: Strictly increasing function
- **Differentiable**: Smooth gradient everywhere

### üìâ Gradient Properties
- **Derivative**: œÉ'(x) = œÉ(x) √ó (1 - œÉ(x))
- **Maximum Gradient**: 0.25 at x = 0
- **Vanishing Gradients**: œÉ'(x) ‚Üí 0 as |x| ‚Üí ‚àû
- **Chain Rule Friendly**: Easy backpropagation computation

## Implementation Details

### üîß Go Layer Integration
```go
// Factory method
func (lf *LayerFactory) CreateSigmoidSpec(name string) LayerSpec {
    return LayerSpec{
        Type:       Sigmoid,
        Name:       name,
        Parameters: map[string]interface{}{},
    }
}

// ModelBuilder method
func (mb *ModelBuilder) AddSigmoid(name string) *ModelBuilder {
    layer := LayerSpec{
        Type:       Sigmoid,
        Name:       name,
        Parameters: map[string]interface{}{},
    }
    return mb.AddLayer(layer)
}
```

### üîÑ ONNX Compatibility
- **Export**: Creates ONNX "Sigmoid" nodes
- **Import**: Converts ONNX "Sigmoid" nodes to go-metal Sigmoid layers
- **Standard Compliance**: ONNX v1.7+ compatibility

### üíæ Checkpoint Support
- **Serialization**: No parameters to save (activation function only)
- **JSON Format**: Layer type and name preservation
- **Model Recovery**: Full architecture restoration

## Use Cases in Practice

### üéØ Binary Classification
```go
// Final layer setup for binary classification
model := builder.
    AddDense(hiddenSize, true, "hidden").
    AddReLU("hidden_activation").           // ReLU for hidden layers
    AddDense(1, true, "output").           // Single output neuron
    AddSigmoid("output_activation").       // Sigmoid for probability
    Compile()

// Loss function: BinaryCrossEntropy expects probabilities [0,1]
loss := training.NewBinaryCrossEntropyLoss()
```

### üß† Legacy Network Architectures
```go
// Pre-ReLU era neural network design
model := builder.
    AddDense(256, true, "layer1").
    AddSigmoid("sigmoid1").               // Traditional activation
    AddDense(128, true, "layer2").
    AddSigmoid("sigmoid2").               // Hidden layer activation
    AddDense(10, true, "output").
    AddSoftmax(-1, "softmax").            // Multi-class output
    Compile()
```

## Performance Characteristics

### ‚ö° GPU Performance
- **Metal Optimized**: Uses Apple's optimized MPS implementation
- **Kernel Fusion**: Automatically fused with adjacent operations
- **Memory Bandwidth**: Minimal memory overhead
- **Throughput**: High-performance sigmoid computation

### üîÑ Training Efficiency
- **Forward Pass**: Single Metal kernel execution
- **Backward Pass**: Automatic MPSGraph differentiation
- **Gradient Computation**: œÉ'(x) = œÉ(x) √ó (1 - œÉ(x))
- **Memory Usage**: In-place computation when possible

## Comparison with Other Activations

| Activation | Range | Gradient | Use Case | Pros | Cons |
|------------|-------|----------|----------|------|------|
| **Sigmoid** | (0, 1) | œÉ'(x) = œÉ(x)(1-œÉ(x)) | Binary classification | Bounded output, smooth | Vanishing gradients |
| **ReLU** | [0, ‚àû) | 1 if x>0, 0 if x‚â§0 | Hidden layers | No vanishing gradients | Dead neurons |
| **Tanh** | (-1, 1) | 1 - tanh¬≤(x) | Hidden layers | Zero-centered | Vanishing gradients |
| **LeakyReLU** | (-‚àû, ‚àû) | 1 if x>0, Œ± if x‚â§0 | Hidden layers | No dead neurons | Unbounded negative |

## Best Practices

### ‚úÖ When to Use Sigmoid
1. **Binary Classification**: Final layer for probability output
2. **Gating Mechanisms**: LSTM/GRU gate functions
3. **Legacy Models**: Reproducing older architectures
4. **Probability Outputs**: When you need [0,1] range

### ‚ùå When to Avoid Sigmoid
1. **Deep Networks**: Vanishing gradient problem
2. **Hidden Layers**: ReLU generally performs better
3. **Multi-class**: Use Softmax instead
4. **Speed Critical**: ReLU is computationally faster

### üéØ Implementation Tips
1. **Initialize Weights Carefully**: Xavier/Glorot initialization works well
2. **Learning Rate**: May need lower learning rates due to gradients
3. **Batch Normalization**: Helps mitigate vanishing gradients
4. **Loss Function**: Use BCEWithLogits for numerical stability

---

**This example demonstrates Go-Metal's comprehensive Sigmoid activation implementation, ready for production use in binary classification and legacy network architectures!** üöÄ