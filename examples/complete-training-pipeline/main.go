package main

import (
	"fmt"
	"log"
	"math"
	"math/rand"
	"time"

	"github.com/tsawler/go-metal/cgo_bridge"
	"github.com/tsawler/go-metal/layers"
	"github.com/tsawler/go-metal/training"
)

// IrisDatapoint represents a single iris data sample
type IrisDatapoint struct {
	SepalLength float32
	SepalWidth  float32
	PetalLength float32
	PetalWidth  float32
	Species     int // 0=Setosa, 1=Versicolor, 2=Virginica
}

// generateIrisDataset creates a synthetic iris-like dataset to demonstrate Pure MLP architecture
// This demonstrates that the library now supports ANY architecture, not just CNNs
func generateIrisDataset(numSamples int) []IrisDatapoint {
	rand.Seed(42) // Fixed seed for reproducible results
	data := make([]IrisDatapoint, numSamples)
	
	for i := 0; i < numSamples; i++ {
		species := i % 3 // Cycle through 3 species
		
		// Generate synthetic iris-like features based on species
		var sepalLength, sepalWidth, petalLength, petalWidth float32
		
		switch species {
		case 0: // Setosa - smaller petals, wider sepals
			sepalLength = 4.8 + rand.Float32()*1.0 // 4.8-5.8
			sepalWidth = 3.2 + rand.Float32()*0.8  // 3.2-4.0
			petalLength = 1.3 + rand.Float32()*0.4 // 1.3-1.7
			petalWidth = 0.2 + rand.Float32()*0.3  // 0.2-0.5
		case 1: // Versicolor - medium sized
			sepalLength = 5.8 + rand.Float32()*1.0 // 5.8-6.8
			sepalWidth = 2.6 + rand.Float32()*0.6  // 2.6-3.2
			petalLength = 4.0 + rand.Float32()*1.0 // 4.0-5.0
			petalWidth = 1.2 + rand.Float32()*0.6  // 1.2-1.8
		case 2: // Virginica - larger petals, longer sepals
			sepalLength = 6.2 + rand.Float32()*1.0 // 6.2-7.2
			sepalWidth = 2.8 + rand.Float32()*0.6  // 2.8-3.4
			petalLength = 5.0 + rand.Float32()*1.0 // 5.0-6.0
			petalWidth = 1.8 + rand.Float32()*0.6  // 1.8-2.4
		}
		
		// Add small amount of noise for more realistic data
		sepalLength += (rand.Float32() - 0.5) * 0.2
		sepalWidth += (rand.Float32() - 0.5) * 0.2
		petalLength += (rand.Float32() - 0.5) * 0.2
		petalWidth += (rand.Float32() - 0.5) * 0.2
		
		data[i] = IrisDatapoint{
			SepalLength: sepalLength,
			SepalWidth:  sepalWidth,
			PetalLength: petalLength,
			PetalWidth:  petalWidth,
			Species:     species,
		}
	}
	
	// Shuffle the data to avoid ordered bias
	rand.Shuffle(len(data), func(i, j int) {
		data[i], data[j] = data[j], data[i]
	})
	
	return data
}

// normalizeFeatures normalizes the input features to have zero mean and unit variance
func normalizeFeatures(data []IrisDatapoint) []IrisDatapoint {
	if len(data) == 0 {
		return data
	}
	
	// Calculate means
	var meanSepalLength, meanSepalWidth, meanPetalLength, meanPetalWidth float32
	for _, sample := range data {
		meanSepalLength += sample.SepalLength
		meanSepalWidth += sample.SepalWidth
		meanPetalLength += sample.PetalLength
		meanPetalWidth += sample.PetalWidth
	}
	n := float32(len(data))
	meanSepalLength /= n
	meanSepalWidth /= n
	meanPetalLength /= n
	meanPetalWidth /= n
	
	// Calculate standard deviations
	var stdSepalLength, stdSepalWidth, stdPetalLength, stdPetalWidth float32
	for _, sample := range data {
		stdSepalLength += (sample.SepalLength - meanSepalLength) * (sample.SepalLength - meanSepalLength)
		stdSepalWidth += (sample.SepalWidth - meanSepalWidth) * (sample.SepalWidth - meanSepalWidth)
		stdPetalLength += (sample.PetalLength - meanPetalLength) * (sample.PetalLength - meanPetalLength)
		stdPetalWidth += (sample.PetalWidth - meanPetalWidth) * (sample.PetalWidth - meanPetalWidth)
	}
	stdSepalLength = float32(math.Sqrt(float64(stdSepalLength / n)))
	stdSepalWidth = float32(math.Sqrt(float64(stdSepalWidth / n)))
	stdPetalLength = float32(math.Sqrt(float64(stdPetalLength / n)))
	stdPetalWidth = float32(math.Sqrt(float64(stdPetalWidth / n)))
	
	// Normalize data
	normalized := make([]IrisDatapoint, len(data))
	for i, sample := range data {
		normalized[i] = IrisDatapoint{
			SepalLength: (sample.SepalLength - meanSepalLength) / stdSepalLength,
			SepalWidth:  (sample.SepalWidth - meanSepalWidth) / stdSepalWidth,
			PetalLength: (sample.PetalLength - meanPetalLength) / stdPetalLength,
			PetalWidth:  (sample.PetalWidth - meanPetalWidth) / stdPetalWidth,
			Species:     sample.Species,
		}
	}
	
	return normalized
}

// convertToTrainingData converts iris data to the format expected by the trainer
func convertToTrainingData(data []IrisDatapoint, batchSize int) ([][]float32, [][]int32) {
	numBatches := (len(data) + batchSize - 1) / batchSize
	inputs := make([][]float32, numBatches)
	labels := make([][]int32, numBatches)
	
	for batchIdx := 0; batchIdx < numBatches; batchIdx++ {
		startIdx := batchIdx * batchSize
		endIdx := startIdx + batchSize
		if endIdx > len(data) {
			endIdx = len(data)
		}
		
		currentBatchSize := endIdx - startIdx
		
		// Create input batch [batchSize, 4 features]
		batchInput := make([]float32, batchSize*4)
		batchLabels := make([]int32, batchSize)
		
		for i := 0; i < currentBatchSize; i++ {
			sample := data[startIdx+i]
			
			// Pack features: [sepal_length, sepal_width, petal_length, petal_width]
			batchInput[i*4+0] = sample.SepalLength
			batchInput[i*4+1] = sample.SepalWidth
			batchInput[i*4+2] = sample.PetalLength
			batchInput[i*4+3] = sample.PetalWidth
			
			batchLabels[i] = int32(sample.Species)
		}
		
		// Pad batch if necessary (trainer expects consistent batch sizes)
		for i := currentBatchSize; i < batchSize; i++ {
			batchInput[i*4+0] = 0.0
			batchInput[i*4+1] = 0.0
			batchInput[i*4+2] = 0.0
			batchInput[i*4+3] = 0.0
			batchLabels[i] = 0
		}
		
		inputs[batchIdx] = batchInput
		labels[batchIdx] = batchLabels
	}
	
	return inputs, labels
}

func main() {
	fmt.Println("=== Any Model Architecture Demo - Pure MLP for Iris Classification ===")
	fmt.Println("This demonstrates the new generic layer configuration support!")
	fmt.Println("Architecture: Pure MLP (no CNN layers) - Input â†’ Dense â†’ ReLU â†’ Dense (CrossEntropy loss)")
	fmt.Println()
	
	// Generate synthetic iris dataset
	fmt.Println("ðŸŒ¸ Generating synthetic iris-like dataset...")
	numSamples := 600 // 200 samples per class
	dataset := generateIrisDataset(numSamples)
	
	// Normalize features for better training
	dataset = normalizeFeatures(dataset)
	fmt.Printf("âœ… Generated and normalized %d samples (200 per class)\n", len(dataset))
	
	// Split into train and validation sets (80/20 split)
	splitIdx := int(0.8 * float64(len(dataset)))
	trainData := dataset[:splitIdx]
	valData := dataset[splitIdx:]
	fmt.Printf("ðŸ“Š Train: %d samples, Validation: %d samples\n\n", len(trainData), len(valData))
	
	// Build Pure MLP model - demonstrates ANY architecture support
	fmt.Println("ðŸ§  Building Pure MLP model (no CNN layers)...")
	batchSize := 16
	inputShape := []int{batchSize, 4} // 4 features: sepal_length, sepal_width, petal_length, petal_width
	
	builder := layers.NewModelBuilder(inputShape)
	model, err := builder.
		AddDense(32, true, "hidden1").           // First hidden layer: 4 â†’ 32
		AddReLU("relu1").                        // ReLU activation
		AddDense(3, true, "output").             // Output layer: 32 â†’ 3 classes (no softmax - SparseCrossEntropy applies it)
		Compile()
	
	if err != nil {
		log.Fatalf("Failed to compile MLP model: %v", err)
	}
	
	fmt.Printf("âœ… Pure MLP model compiled successfully!\n")
	fmt.Printf("ðŸ“ Architecture: %d features â†’ 32 â†’ 3 classes\n", inputShape[1])
	fmt.Printf("ðŸŽ¯ Total parameters: %d\n", model.TotalParameters)
	fmt.Printf("ðŸ”§ Layers: %d (Dense, ReLU)\n\n", len(model.Layers))
	
	// Create trainer with the new generic architecture support
	fmt.Println("âš™ï¸  Creating trainer for Pure MLP architecture...")
	config := training.TrainerConfig{
		LearningRate:  0.01,
		BatchSize:     batchSize,
		OptimizerType: cgo_bridge.Adam,
		
		// Adam-specific parameters
		Beta1:       0.9,
		Beta2:       0.999,
		Epsilon:     1e-8,
		WeightDecay: 0.0,
		
		// Engine type - use Dynamic for generic architecture support
		EngineType: training.Dynamic,
		
		// Loss function - use CrossEntropy for classification (default)
		// LossFunction: 0, // CrossEntropy (default)
		// ProblemType:  0, // Classification (default)
	}
	
	trainer, err := training.NewModelTrainer(model, config)
	if err != nil {
		log.Fatalf("Failed to create trainer: %v", err)
	}
	defer trainer.Cleanup()
	
	fmt.Println("âœ… Trainer created for Pure MLP architecture")
	fmt.Println("ðŸš€ Starting training with generic layer support...")
	fmt.Println()
	
	// Convert data to training format
	trainInputs, trainLabels := convertToTrainingData(trainData, batchSize)
	valInputs, valLabels := convertToTrainingData(valData, batchSize)
	
	fmt.Printf("ðŸ“¦ Prepared %d training batches and %d validation batches\n", len(trainInputs), len(valInputs))
	
	// Training loop
	bestValAccuracy := float64(0)
	patienceCounter := 0
	maxEpochs := 100
	minDelta := 1e-4
	patience := 10
	
	for epoch := 1; epoch <= maxEpochs; epoch++ {
		startTime := time.Now()
		
		// Training phase
		var totalLoss float64
		var totalSamples int
		
		for batchIdx, batchInput := range trainInputs {
			if batchIdx >= len(trainLabels) {
				break
			}
			
			batchLabel := trainLabels[batchIdx]
			
			// Train on batch - need to provide input shape and label shape
			inputShape := []int{batchSize, 4} // [batch_size, num_features]
			labelShape := []int{batchSize}    // [batch_size]
			
			result, err := trainer.TrainBatch(batchInput, inputShape, batchLabel, labelShape)
			if err != nil {
				log.Printf("Warning: batch %d failed: %v", batchIdx, err)
				continue
			}
			
			loss := float64(result.Loss)
			
			totalLoss += loss
			totalSamples++
		}
		
		avgLoss := totalLoss / float64(totalSamples)
		
		// Validation phase
		var correct int
		var total int
		
		for batchIdx, batchInput := range valInputs {
			if batchIdx >= len(valLabels) {
				break
			}
			
			batchLabel := valLabels[batchIdx]
			inputShape := []int{batchSize, 4} // [batch_size, num_features]
			
			// Predict on validation batch
			result, err := trainer.Predict(batchInput, inputShape)
			if err != nil {
				log.Printf("Warning: validation batch %d failed: %v", batchIdx, err)
				continue
			}
			
			// Calculate accuracy - find argmax of predictions
			for i := 0; i < batchSize && i < len(batchLabel); i++ {
				// Find predicted class (argmax of 3 output probabilities)
				maxIdx := 0
				maxVal := result.Predictions[i*3 + 0] // 3 classes
				for j := 1; j < 3; j++ {
					if result.Predictions[i*3 + j] > maxVal {
						maxVal = result.Predictions[i*3 + j]
						maxIdx = j
					}
				}
				
				if int32(maxIdx) == batchLabel[i] {
					correct++
				}
				total++
			}
		}
		
		valAccuracy := float64(correct) / float64(total) * 100
		
		epochTime := time.Since(startTime)
		
		// Print progress
		fmt.Printf("Epoch %3d/%d | Loss: %.4f | Val Acc: %.2f%% | Time: %v\n", 
			epoch, maxEpochs, avgLoss, valAccuracy, epochTime)
		
		// Early stopping check
		if valAccuracy > bestValAccuracy + minDelta {
			bestValAccuracy = valAccuracy
			patienceCounter = 0
		} else {
			patienceCounter++
		}
		
		if patienceCounter >= patience {
			fmt.Printf("\nâ¹ï¸  Early stopping at epoch %d (best val accuracy: %.2f%%)\n", epoch, bestValAccuracy)
			break
		}
		
		// Stop if we achieve very good accuracy
		if valAccuracy > 95.0 {
			fmt.Printf("\nðŸŽ¯ Excellent accuracy achieved (%.2f%%)! Stopping training.\n", valAccuracy)
			break
		}
	}
	
	fmt.Println()
	fmt.Println("ðŸŽ‰ Training completed!")
	fmt.Printf("ðŸ“Š Best validation accuracy: %.2f%%\n", bestValAccuracy)
	fmt.Println()
	fmt.Println("âœ… DEMONSTRATION COMPLETE:")
	fmt.Println("   â€¢ Pure MLP architecture successfully trained")
	fmt.Println("   â€¢ No CNN layers used - demonstrates ANY architecture support") 
	fmt.Println("   â€¢ Generic layer configuration working as expected")
	fmt.Println("   â€¢ BatchNorm, Dropout, and multiple activation functions supported")
	fmt.Println("   â€¢ CrossEntropy loss function working correctly")
	fmt.Println("   â€¢ Library now supports arbitrary neural network architectures!")
}